"""
ALIKED Building Blocks - ç°¡ç•¥åŒ–ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰
=========================================

ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ:
- ConvBlock: åŸºæœ¬ç•³ã¿è¾¼ã¿ãƒ–ãƒ­ãƒƒã‚¯
- ResBlock: æ®‹å·®ãƒ–ãƒ­ãƒƒã‚¯ (Deformable Convã‚ªãƒ—ã‚·ãƒ§ãƒ³)
- SDDH: Sparse Deformable Descriptor Head (KEY INNOVATION)
- DeformableConv2d: å¤‰å½¢å¯èƒ½ç•³ã¿è¾¼ã¿
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional


class ConvBlock(nn.Module):
    """
    åŸºæœ¬ç•³ã¿è¾¼ã¿ãƒ–ãƒ­ãƒƒã‚¯

    æ§‹æˆ:
      - Conv3x3 â†’ BN â†’ SELU
      - Conv3x3 â†’ BN â†’ SELU
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        stride: int = 1
    ):
        super().__init__()

        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3,
                     stride=stride, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.SELU(),

            nn.Conv2d(out_channels, out_channels, kernel_size=3,
                     stride=1, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.SELU()
        )

    def forward(self, x):
        return self.conv(x)


class ResBlock(nn.Module):
    """
    æ®‹å·®ãƒ–ãƒ­ãƒƒã‚¯

    Block3/4ã§ä½¿ç”¨:
      - Deformable Convä½¿ç”¨å¯èƒ½
      - Skip connection
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        use_dcn: bool = False
    ):
        super().__init__()

        if use_dcn:
            # Deformable Convolutionä½¿ç”¨
            self.conv1 = DeformableConv2d(in_channels, out_channels)
            self.conv2 = DeformableConv2d(out_channels, out_channels)
        else:
            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)
            self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)

        self.bn1 = nn.BatchNorm2d(out_channels)
        self.bn2 = nn.BatchNorm2d(out_channels)

        # Skip connection
        self.skip = nn.Identity() if in_channels == out_channels else \
                    nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)

    def forward(self, x):
        identity = self.skip(x)

        out = self.conv1(x)
        out = self.bn1(out)
        out = F.selu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        out = out + identity
        out = F.selu(out)

        return out


class DeformableConv2d(nn.Module):
    """
    Deformable Convolution (DCNv2é¢¨)

    é€šå¸¸ã®ç•³ã¿è¾¼ã¿ã¨ç•°ãªã‚Š:
      - å„ãƒ”ã‚¯ã‚»ãƒ«ã§å­¦ç¿’å¯èƒ½ãªã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’æ¨å®š
      - ã‚ªãƒ•ã‚»ãƒƒãƒˆã«åŸºã¥ã„ã¦æŸ”è»Ÿã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
      - å¹¾ä½•å­¦çš„å¤‰æ›ã«å¯¾ã™ã‚‹ä¸å¤‰æ€§ã‚’ç²å¾—

    å‡¦ç†:
      1. ã‚ªãƒ•ã‚»ãƒƒãƒˆæ¨å®š: Conv3x3 â†’ offsets (2 * K^2 channels)
      2. Deformable sampling: grid_sample
      3. ç•³ã¿è¾¼ã¿é©ç”¨
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int = 3,
        stride: int = 1,
        padding: int = 1
    ):
        super().__init__()

        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding

        # ã‚ªãƒ•ã‚»ãƒƒãƒˆæ¨å®šãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.offset_conv = nn.Conv2d(
            in_channels,
            2 * kernel_size * kernel_size,  # (dx, dy) for each position
            kernel_size=kernel_size,
            stride=stride,
            padding=padding
        )

        # é€šå¸¸ã®ç•³ã¿è¾¼ã¿
        self.regular_conv = nn.Conv2d(
            in_channels,
            out_channels,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
            bias=False
        )

    def forward(self, x):
        """
        å…¥åŠ›:
            x: (B, C_in, H, W)

        å‡ºåŠ›:
            out: (B, C_out, H', W')
        """

        # ã‚ªãƒ•ã‚»ãƒƒãƒˆæ¨å®š
        offsets = self.offset_conv(x)
        # offsets: (B, 2*K*K, H', W')

        # Deformable sampling
        x_offset = self._deform_sampling(x, offsets)
        # x_offset: (B, C_in, H', W')

        # ç•³ã¿è¾¼ã¿é©ç”¨
        out = self.regular_conv(x_offset)

        return out

    def _deform_sampling(self, x, offsets):
        """
        å¤‰å½¢å¯èƒ½ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

        ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¦ã€
        ã‚ªãƒ•ã‚»ãƒƒãƒˆä½ç½®ã‹ã‚‰ç‰¹å¾´ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        """
        B, C, H, W = x.shape
        K = self.kernel_size

        # ã‚°ãƒªãƒƒãƒ‰ç”Ÿæˆ (ç°¡ç•¥åŒ–ç‰ˆ)
        # å®Ÿè£…ã§ã¯ torch.nn.functional.grid_sample ã‚’ä½¿ç”¨
        # ã“ã“ã§ã¯æ¦‚å¿µçš„ãªå‡¦ç†ã®ã¿è¨˜è¼‰

        return x  # ç°¡ç•¥åŒ–ã®ãŸã‚å…ƒã®ç‰¹å¾´ã‚’è¿”ã™


class SDDH(nn.Module):
    """
    Sparse Deformable Descriptor Head (SDDH)

    ğŸ”‘ ALIKEDã®æœ€å¤§ã®ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³:
    ========================================

    å¾“æ¥æ‰‹æ³• (DMH: Descriptor Map Head):
      - å¯†ãªè¨˜è¿°å­ãƒãƒƒãƒ—ã‚’å…¨ä½“ã§è¨ˆç®—
      - ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆä½ç½®ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
      - è¨ˆç®—é‡: O(H Ã— W Ã— C^2)
      - ãƒ¡ãƒ¢ãƒª: å¤šå¤§

    SDDH:
      - ã‚¹ãƒ‘ãƒ¼ã‚¹ãªã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®ã¿ã§è¨˜è¿°å­æŠ½å‡º
      - å„ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã§å¤‰å½¢å¯èƒ½ãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ä½ç½®ã‚’å­¦ç¿’
      - è¨ˆç®—é‡: O(N Ã— M Ã— C) where N=ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ•°, M=ã‚µãƒ³ãƒ—ãƒ«ä½ç½®æ•°
      - ãƒ¡ãƒ¢ãƒª: å¤§å¹…å‰Šæ¸› (50å€ä»¥ä¸Š)

    å‡¦ç†ãƒ•ãƒ­ãƒ¼:
      1. ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆå‘¨è¾ºã®KÃ—Kãƒ‘ãƒƒãƒã‚’æŠ½å‡º
      2. ãƒ‘ãƒƒãƒã‹ã‚‰Mãƒ‡formableã‚µãƒ³ãƒ—ãƒ«ä½ç½®ã‚’æ¨å®š
      3. ç‰¹å¾´ãƒãƒƒãƒ—ã‹ã‚‰å¤‰å½¢å¯èƒ½ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
      4. ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‰¹å¾´ã‚’é›†ç´„ã—ã¦è¨˜è¿°å­ç”Ÿæˆ
    """

    def __init__(
        self,
        in_dim: int = 128,
        out_dim: int = 128,
        M: int = 16,           # ãƒ‡formableã‚µãƒ³ãƒ—ãƒ«ä½ç½®æ•°
        K: int = 3             # ãƒ‘ãƒƒãƒã‚µã‚¤ã‚º
    ):
        super().__init__()

        self.M = M
        self.K = K
        self.in_dim = in_dim
        self.out_dim = out_dim

        # ========================================
        # Offsetæ¨å®šãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        # ========================================
        # KÃ—Kãƒ‘ãƒƒãƒ â†’ Må€‹ã®2Dã‚ªãƒ•ã‚»ãƒƒãƒˆ

        self.offset_net = nn.Sequential(
            nn.Conv2d(in_dim, in_dim, kernel_size=K, padding=0),  # No padding
            nn.SELU(),
            nn.Conv2d(in_dim, 2 * M, kernel_size=1)
        )


        # ========================================
        # ç‰¹å¾´ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
        # ========================================
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸç‰¹å¾´ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰

        self.feature_encoder = nn.Sequential(
            nn.Conv2d(in_dim, out_dim, kernel_size=1),
            nn.SELU()
        )


        # ========================================
        # è¨˜è¿°å­é›†ç´„
        # ========================================
        # Må€‹ã®ç‰¹å¾´ â†’ 1ã¤ã®è¨˜è¿°å­

        # Learnable weights for aggregation
        self.agg_weights = nn.Parameter(torch.randn(M, out_dim, out_dim))

        # ã¾ãŸã¯ Conv1x1ãƒ™ãƒ¼ã‚¹ã®é›†ç´„
        self.agg_conv = nn.Conv2d(out_dim, out_dim, kernel_size=1)


    def forward(
        self,
        features: torch.Tensor,
        keypoints: torch.Tensor
    ) -> torch.Tensor:
        """
        Sparse Deformable DescriptoræŠ½å‡º

        å…¥åŠ›:
            features: (B, in_dim, H, W) - é›†ç´„ã•ã‚ŒãŸç‰¹å¾´ãƒãƒƒãƒ—
            keypoints: (B, N, 2) - ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆåº§æ¨™ [x, y]

        å‡ºåŠ›:
            descriptors: (B, N, out_dim) - è¨˜è¿°å­
        """

        B, C, H, W = features.shape
        B, N, _ = keypoints.shape

        # ========================================
        # Step 1: KÃ—Kãƒ‘ãƒƒãƒæŠ½å‡º
        # ========================================

        # å„ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆå‘¨è¾ºã®KÃ—Kãƒ‘ãƒƒãƒã‚’æŠ½å‡º
        patches = self._extract_patches(features, keypoints, self.K)
        # patches: (B*N, in_dim, K, K)


        # ========================================
        # Step 2: Deformableã‚µãƒ³ãƒ—ãƒ«ä½ç½®æ¨å®š
        # ========================================

        offsets = self.offset_net(patches)
        # offsets: (B*N, 2*M, 1, 1)

        offsets = offsets.view(B * N, self.M, 2)
        # offsets: (B*N, M, 2)

        # ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’ã‚¯ãƒ©ãƒ³ãƒ— (æ¥µç«¯ãªå¤‰ä½ã‚’é˜²æ­¢)
        max_offset = max(H, W) / 4
        offsets = torch.clamp(offsets, -max_offset, max_offset)


        # ========================================
        # Step 3: Deformable Sampling
        # ========================================

        # ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆåº§æ¨™ + ã‚ªãƒ•ã‚»ãƒƒãƒˆ
        keypoints_flat = keypoints.view(B * N, 2)  # (B*N, 2)
        sample_positions = keypoints_flat.unsqueeze(1) + offsets
        # sample_positions: (B*N, M, 2)

        # ç‰¹å¾´ãƒãƒƒãƒ—ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        sampled_features = self._sample_features(
            features,
            sample_positions.view(B, N, self.M, 2)
        )
        # sampled_features: (B, N, M, in_dim)


        # ========================================
        # Step 4: ç‰¹å¾´ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        # ========================================

        # (B, N, M, in_dim) â†’ (B*N*M, in_dim, 1, 1) for conv
        sampled_features = sampled_features.reshape(B * N * self.M, self.in_dim, 1, 1)

        encoded = self.feature_encoder(sampled_features)
        # encoded: (B*N*M, out_dim, 1, 1)

        encoded = encoded.squeeze(-1).squeeze(-1)
        # encoded: (B*N*M, out_dim)

        encoded = encoded.view(B, N, self.M, self.out_dim)
        # encoded: (B, N, M, out_dim)


        # ========================================
        # Step 5: è¨˜è¿°å­é›†ç´„
        # ========================================

        # Method 1: Learnable weighted sum
        # descriptors = torch.einsum('bnmc,mcd->bnd', encoded, self.agg_weights)

        # Method 2: Simple average
        descriptors = encoded.mean(dim=2)
        # descriptors: (B, N, out_dim)

        # L2æ­£è¦åŒ–
        descriptors = F.normalize(descriptors, p=2, dim=-1)

        return descriptors


    def _extract_patches(
        self,
        features: torch.Tensor,
        keypoints: torch.Tensor,
        patch_size: int
    ) -> torch.Tensor:
        """
        ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆå‘¨è¾ºã®ãƒ‘ãƒƒãƒæŠ½å‡º

        å…¥åŠ›:
            features: (B, C, H, W)
            keypoints: (B, N, 2)
            patch_size: int

        å‡ºåŠ›:
            patches: (B*N, C, patch_size, patch_size)
        """

        B, C, H, W = features.shape
        B, N, _ = keypoints.shape

        # Grid sampleç”¨ã«åº§æ¨™ã‚’æ­£è¦åŒ– [-1, 1]
        kpts_norm = keypoints.clone()
        kpts_norm[:, :, 0] = 2.0 * kpts_norm[:, :, 0] / (W - 1) - 1.0
        kpts_norm[:, :, 1] = 2.0 * kpts_norm[:, :, 1] / (H - 1) - 1.0

        # ãƒ‘ãƒƒãƒã‚°ãƒªãƒƒãƒ‰ç”Ÿæˆ
        half = patch_size // 2
        grid_y, grid_x = torch.meshgrid(
            torch.arange(-half, half + 1, dtype=torch.float32, device=features.device),
            torch.arange(-half, half + 1, dtype=torch.float32, device=features.device),
            indexing='ij'
        )

        # æ­£è¦åŒ–
        grid_x = 2.0 * grid_x / (W - 1)
        grid_y = 2.0 * grid_y / (H - 1)

        grid = torch.stack([grid_x, grid_y], dim=-1)
        # grid: (patch_size, patch_size, 2)

        # ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆä½ç½®ã«ç§»å‹•
        kpts_norm = kpts_norm.view(B, N, 1, 1, 2)
        grid = grid.view(1, 1, patch_size, patch_size, 2)

        sampling_grid = kpts_norm + grid
        # sampling_grid: (B, N, patch_size, patch_size, 2)

        # Grid sample
        features_expanded = features.unsqueeze(1).expand(B, N, C, H, W)
        features_flat = features_expanded.reshape(B * N, C, H, W)

        sampling_grid_flat = sampling_grid.reshape(B * N, patch_size, patch_size, 2)

        patches = F.grid_sample(
            features_flat,
            sampling_grid_flat,
            mode='bilinear',
            align_corners=False
        )
        # patches: (B*N, C, patch_size, patch_size)

        return patches


    def _sample_features(
        self,
        features: torch.Tensor,
        sample_positions: torch.Tensor
    ) -> torch.Tensor:
        """
        ä»»æ„ä½ç½®ã‹ã‚‰ç‰¹å¾´ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

        å…¥åŠ›:
            features: (B, C, H, W)
            sample_positions: (B, N, M, 2) - [x, y]åº§æ¨™

        å‡ºåŠ›:
            sampled: (B, N, M, C)
        """

        B, C, H, W = features.shape
        B, N, M, _ = sample_positions.shape

        # æ­£è¦åŒ– [-1, 1]
        pos_norm = sample_positions.clone()
        pos_norm[:, :, :, 0] = 2.0 * pos_norm[:, :, :, 0] / (W - 1) - 1.0
        pos_norm[:, :, :, 1] = 2.0 * pos_norm[:, :, :, 1] / (H - 1) - 1.0

        # Grid sample
        features_expanded = features.unsqueeze(1).expand(B, N, C, H, W)
        features_flat = features_expanded.reshape(B * N, C, H, W)

        pos_norm_flat = pos_norm.reshape(B * N, M, 1, 2)

        sampled = F.grid_sample(
            features_flat,
            pos_norm_flat,
            mode='bilinear',
            align_corners=False
        )
        # sampled: (B*N, C, M, 1)

        sampled = sampled.squeeze(-1).permute(0, 2, 1)
        # sampled: (B*N, M, C)

        sampled = sampled.view(B, N, M, C)

        return sampled


# ============================================
# ä½¿ç”¨ä¾‹
# ============================================

def example_sddh():
    """SDDHä½¿ç”¨ä¾‹"""

    # SDDHä½œæˆ
    sddh = SDDH(
        in_dim=128,
        out_dim=128,
        M=16,    # 16å€‹ã®deformableã‚µãƒ³ãƒ—ãƒ«ä½ç½®
        K=3      # 3Ã—3ãƒ‘ãƒƒãƒ
    )

    # ãƒ€ãƒŸãƒ¼å…¥åŠ›
    features = torch.randn(2, 128, 160, 120)  # (B, dim, H, W)
    keypoints = torch.randint(0, 100, (2, 500, 2)).float()  # (B, N, 2)

    # è¨˜è¿°å­æŠ½å‡º
    descriptors = sddh(features, keypoints)

    print(f"Features: {features.shape}")
    print(f"Keypoints: {keypoints.shape}")
    print(f"Descriptors: {descriptors.shape}")  # (2, 500, 128)

    # åŠ¹ç‡æ€§ã®ç¢ºèª
    print("\n=== Efficiency Comparison ===")
    print("Dense Descriptor Map (DMH):")
    print(f"  Operations: H Ã— W Ã— C^2 = 160 Ã— 120 Ã— 128^2 = 314M")
    print(f"  Memory: H Ã— W Ã— C = 160 Ã— 120 Ã— 128 = 2.5MB")

    print("\nSparse Deformable Descriptor (SDDH):")
    print(f"  Operations: N Ã— M Ã— C = 500 Ã— 16 Ã— 128 = 1.0M")
    print(f"  Memory: N Ã— C = 500 Ã— 128 = 64KB")
    print(f"  Speedup: ~300x faster!")


if __name__ == "__main__":
    example_sddh()
