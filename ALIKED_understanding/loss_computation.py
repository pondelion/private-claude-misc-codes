"""
ALIKED Loss Computation - ç°¡ç•¥åŒ–ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰
=========================================

5ã¤ã®æå¤±é–¢æ•°:
1. Reprojection Loss - ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆä½ç½®ã®å¹¾ä½•çš„æ•´åˆæ€§
2. Peaky Loss - ã‚¹ã‚³ã‚¢ãƒãƒƒãƒ—ã®é‹­ã•
3. Sparse NRE Loss - ã‚¹ãƒ‘ãƒ¼ã‚¹è¨˜è¿°å­ã®ãƒãƒƒãƒãƒ³ã‚° (KEY INNOVATION)
4. Reliable Loss - è¨˜è¿°å­ã®ä¿¡é ¼æ€§
5. (Optional) Triplet Loss - Hard negative mining
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple

class ALIKEDLossWrapper(nn.Module):
    """
    ALIKED æå¤±è¨ˆç®—ãƒ©ãƒƒãƒ‘ãƒ¼

    æå¤±é‡ã¿ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ):
    - w_rp: 1.0  (Reprojection Loss)
    - w_pk: 0.5  (Peaky Loss)
    - w_ds: 5.0  (Sparse NRE Loss)
    - w_re: 1.0  (Reliable Loss)
    - w_triplet: 0.0  (é€šå¸¸ã¯æœªä½¿ç”¨)
    """

    def __init__(
        self,
        w_rp: float = 1.0,
        w_pk: float = 0.5,
        w_ds: float = 5.0,
        w_re: float = 1.0,
        w_triplet: float = 0.0,
        tdes: float = 0.1,      # Descriptor temperature
        trel: float = 1.0       # Reliability temperature
    ):
        super().__init__()

        self.w_rp = w_rp
        self.w_pk = w_pk
        self.w_ds = w_ds
        self.w_re = w_re
        self.w_triplet = w_triplet

        self.tdes = tdes
        self.trel = trel

    def forward(
        self,
        outputs_a: Dict[str, torch.Tensor],
        outputs_b: Dict[str, torch.Tensor],
        homography_ab: torch.Tensor,
        depth_a: torch.Tensor = None,
        R_ab: torch.Tensor = None,
        t_ab: torch.Tensor = None
    ) -> Dict[str, torch.Tensor]:
        """
        æå¤±è¨ˆç®—

        å…¥åŠ›:
            outputs_a: Image Aã®å‡ºåŠ›
                {
                    'keypoints': (B, N_a, 2)
                    'descriptors': (B, N_a, dim)
                    'scores': (B, N_a)
                    'score_map': (B, 1, H, W)
                }
            outputs_b: Image Bã®å‡ºåŠ› (åŒæ§˜)
            homography_ab: (B, 3, 3) - Homographyå¤‰æ› (Homographyãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨)
            depth_a: (B, H, W) - Depth map (Perspectiveãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨)
            R_ab, t_ab: Rotation & translation (Perspectiveãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨)

        å‡ºåŠ›:
            losses: {
                'loss_rp': scalar
                'loss_pk': scalar
                'loss_ds': scalar
                'loss_re': scalar
                'total_loss': scalar
            }
        """

        # ========================================
        # 1. Reprojection Loss
        # ========================================
        loss_rp = self._reprojection_loss(
            outputs_a, outputs_b,
            homography_ab, depth_a, R_ab, t_ab
        )

        # ========================================
        # 2. Peaky Loss (Score Dispersity)
        # ========================================
        loss_pk = self._peaky_loss(outputs_a, outputs_b)

        # ========================================
        # 3. Sparse NRE Loss (Descriptor Matching)
        # ========================================
        loss_ds = self._sparse_nre_loss(
            outputs_a, outputs_b,
            homography_ab, depth_a, R_ab, t_ab
        )

        # ========================================
        # 4. Reliable Loss
        # ========================================
        loss_re = self._reliable_loss(
            outputs_a, outputs_b,
            homography_ab, depth_a, R_ab, t_ab
        )

        # ========================================
        # Total Loss
        # ========================================
        total_loss = (
            self.w_rp * loss_rp +
            self.w_pk * loss_pk +
            self.w_ds * loss_ds +
            self.w_re * loss_re
        )

        return {
            'loss_rp': loss_rp,
            'loss_pk': loss_pk,
            'loss_ds': loss_ds,
            'loss_re': loss_re,
            'total_loss': total_loss
        }

    def _reprojection_loss(
        self,
        outputs_a: Dict,
        outputs_b: Dict,
        H_ab: torch.Tensor,
        depth_a: torch.Tensor = None,
        R_ab: torch.Tensor = None,
        t_ab: torch.Tensor = None
    ) -> torch.Tensor:
        """
        Reprojection Location Loss

        ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®å¹¾ä½•çš„æ•´åˆæ€§ã‚’ä¿è¨¼:
        - pAã‚’Image Bã«æŠ•å½± â†’ pAB
        - pBã«æœ€ã‚‚è¿‘ã„ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã¨ãƒãƒƒãƒãƒ³ã‚°
        - é€†æ–¹å‘ã‚‚åŒæ§˜
        - åŒæ–¹å‘ã®è·é›¢ã‚’æœ€å°åŒ–

        æ•°å¼:
        L_rp = 1/2 * (||pA - pBA|| + ||pB - pAB||)

        å…¥åŠ›:
            outputs_a, outputs_b: ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæƒ…å ±
            H_ab: (B, 3, 3) - Homography
            (ã¾ãŸã¯ depth_a, R_ab, t_ab for perspective)

        å‡ºåŠ›:
            loss: scalar
        """

        kpts_a = outputs_a['keypoints']  # (B, N_a, 2)
        kpts_b = outputs_b['keypoints']  # (B, N_b, 2)

        B = kpts_a.shape[0]

        total_loss = 0.0
        num_matches = 0

        for b in range(B):
            # ========================================
            # A â†’ B æŠ•å½±
            # ========================================
            kpts_a_warped = self._warp_keypoints(
                kpts_a[b],
                H_ab[b] if H_ab is not None else None,
                depth_a[b] if depth_a is not None else None,
                R_ab[b] if R_ab is not None else None,
                t_ab[b] if t_ab is not None else None
            )
            # kpts_a_warped: (N_a, 2)

            # æœ€è¿‘å‚ãƒãƒƒãƒãƒ³ã‚°
            matches_ab = self._find_nearest_neighbors(
                kpts_a_warped,
                kpts_b[b],
                distance_threshold=5.0  # pixels
            )
            # matches_ab: (M, 2) - [idx_a, idx_b]

            # ========================================
            # B â†’ A æŠ•å½±
            # ========================================
            H_ba = torch.inverse(H_ab[b]) if H_ab is not None else None
            kpts_b_warped = self._warp_keypoints(
                kpts_b[b],
                H_ba,
                None,  # depth not needed for inverse
                R_ab[b].T if R_ab is not None else None,
                -t_ab[b] if t_ab is not None else None
            )

            # Reprojection errorè¨ˆç®—
            for idx_a, idx_b in matches_ab:
                # Forward: A â†’ B
                err_ab = torch.norm(kpts_a_warped[idx_a] - kpts_b[b, idx_b])

                # Backward: B â†’ A
                err_ba = torch.norm(kpts_a[b, idx_a] - kpts_b_warped[idx_b])

                total_loss += (err_ab + err_ba) / 2.0
                num_matches += 1

        if num_matches > 0:
            return total_loss / num_matches
        else:
            return torch.tensor(0.0, device=kpts_a.device)

    def _peaky_loss(
        self,
        outputs_a: Dict,
        outputs_b: Dict
    ) -> torch.Tensor:
        """
        Peaky Loss (Dispersity Peak Loss)

        ã‚¹ã‚³ã‚¢ãƒãƒƒãƒ—ã®é‹­ã•ã‚’å¼·åŒ–:
        - ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆä½ç½®ã§ã‚¹ã‚³ã‚¢ãŒé‹­ããƒ”ãƒ¼ã‚¯ã‚’æŒã¤ã‚ˆã†ã«è¨“ç·´
        - Score dispersity (åˆ†æ•£åº¦) ã‚’æœ€å°åŒ–

        æ•°å¼:
        L_pk = mean(softmax(s_patch) Â· ||p - c||)

        where:
          s_patch: ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆå‘¨è¾ºã®ã‚¹ã‚³ã‚¢ãƒ‘ãƒƒãƒ
          p: ãƒ‘ãƒƒãƒå†…ã®å„ãƒ”ã‚¯ã‚»ãƒ«åº§æ¨™
          c: ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆåº§æ¨™

        å…¥åŠ›:
            outputs_a, outputs_b: ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæƒ…å ±

        å‡ºåŠ›:
            loss: scalar
        """

        score_map_a = outputs_a['score_map']  # (B, 1, H, W)
        kpts_a = outputs_a['keypoints']        # (B, N_a, 2)

        score_map_b = outputs_b['score_map']
        kpts_b = outputs_b['keypoints']

        window_size = 5
        half = window_size // 2

        # ã‚°ãƒªãƒƒãƒ‰ç”Ÿæˆ
        grid_y, grid_x = torch.meshgrid(
            torch.arange(-half, half + 1, dtype=torch.float32, device=score_map_a.device),
            torch.arange(-half, half + 1, dtype=torch.float32, device=score_map_a.device),
            indexing='ij'
        )

        distances = torch.sqrt(grid_x ** 2 + grid_y ** 2)
        # distances: (window_size, window_size)

        def compute_dispersity(score_map, kpts):
            B, _, H, W = score_map.shape
            total_dispersity = 0.0
            count = 0

            for b in range(B):
                for n in range(kpts.shape[1]):
                    x_pix = int(kpts[b, n, 0].item())
                    y_pix = int(kpts[b, n, 1].item())

                    if x_pix < half or x_pix >= W - half or \
                       y_pix < half or y_pix >= H - half:
                        continue

                    # ã‚¹ã‚³ã‚¢ãƒ‘ãƒƒãƒæŠ½å‡º
                    score_patch = score_map[b, 0,
                                           y_pix - half:y_pix + half + 1,
                                           x_pix - half:x_pix + half + 1]

                    # Softmax weights
                    weights = F.softmax(score_patch.flatten(), dim=0)

                    # Dispersity
                    dispersity = (weights * distances.flatten()).sum()

                    total_dispersity += dispersity
                    count += 1

            return total_dispersity / count if count > 0 else 0.0

        loss_a = compute_dispersity(score_map_a, kpts_a)
        loss_b = compute_dispersity(score_map_b, kpts_b)

        return (loss_a + loss_b) / 2.0

    def _sparse_nre_loss(
        self,
        outputs_a: Dict,
        outputs_b: Dict,
        H_ab: torch.Tensor,
        depth_a: torch.Tensor = None,
        R_ab: torch.Tensor = None,
        t_ab: torch.Tensor = None
    ) -> torch.Tensor:
        """
        Sparse Neural Reprojection Error Loss

        ğŸ”‘ ALIKEDã®ä¸»è¦ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³:
        ========================================

        å¾“æ¥ã®NRE Loss (Dense):
        - å¯†ãªè¨˜è¿°å­ãƒãƒƒãƒ—ãŒå¿…è¦
        - 2Dç¢ºç‡ãƒãƒƒãƒ—ã‚’æ§‹ç¯‰
        - Cross-entropy loss

        Sparse NRE Loss:
        - ã‚¹ãƒ‘ãƒ¼ã‚¹è¨˜è¿°å­ã®ã¿ä½¿ç”¨
        - 1Dç¢ºç‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’æ§‹ç¯‰
        - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å¤§å¹…å‰Šæ¸›

        å‡¦ç†:
        1. å¹¾ä½•çš„å¯¾å¿œé–¢ä¿‚ã‹ã‚‰ Reprojection Probability Vector æ§‹ç¯‰
        2. è¨˜è¿°å­é¡ä¼¼åº¦ã‹ã‚‰ Matching Probability Vector æ§‹ç¯‰
        3. 2ã¤ã®ç¢ºç‡ãƒ™ã‚¯ãƒˆãƒ«é–“ã®Cross-Entropyæœ€å°åŒ–

        æ•°å¼:
        q_r(pA, P_B) = binary vector (matching=1, others=0)
        q_m(dA, D_B) = softmax((sim(dA, D_B) - 1) / t_des)
        L_ds = -log(q_m(dA, dB))

        å…¥åŠ›:
            outputs_a, outputs_b: ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆãƒ»è¨˜è¿°å­æƒ…å ±
            H_ab: Homography

        å‡ºåŠ›:
            loss: scalar
        """

        kpts_a = outputs_a['keypoints']      # (B, N_a, 2)
        desc_a = outputs_a['descriptors']    # (B, N_a, dim)

        kpts_b = outputs_b['keypoints']      # (B, N_b, 2)
        desc_b = outputs_b['descriptors']    # (B, N_b, dim)

        B = kpts_a.shape[0]

        total_loss = 0.0
        num_matches = 0

        for b in range(B):
            # ========================================
            # Step 1: å¹¾ä½•çš„å¯¾å¿œã‹ã‚‰Reprojection Probabilityæ§‹ç¯‰
            # ========================================

            # A â†’ B æŠ•å½±
            kpts_a_warped = self._warp_keypoints(
                kpts_a[b],
                H_ab[b] if H_ab is not None else None,
                depth_a[b] if depth_a is not None else None,
                R_ab[b] if R_ab is not None else None,
                t_ab[b] if t_ab is not None else None
            )

            # æœ€è¿‘å‚ãƒãƒƒãƒãƒ³ã‚°
            matches_ab = self._find_nearest_neighbors(
                kpts_a_warped,
                kpts_b[b],
                distance_threshold=5.0
            )

            if len(matches_ab) == 0:
                continue

            # ========================================
            # Step 2: å„ãƒãƒƒãƒãƒšã‚¢ã«å¯¾ã—ã¦Sparse NRE Lossè¨ˆç®—
            # ========================================

            for idx_a, idx_b in matches_ab:
                dA = desc_a[b, idx_a]  # (dim,)
                DB = desc_b[b]          # (N_b, dim)

                # ========================================
                # Matching Similarity Vector
                # ========================================
                # Cosine similarity
                sim = torch.matmul(DB, dA)  # (N_b,)

                # Matching probability vector
                q_m = F.softmax((sim - 1.0) / self.tdes, dim=0)
                # q_m: (N_b,) - å…¨ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã«å¯¾ã™ã‚‹ç¢ºç‡

                # ========================================
                # Loss: -log(q_m[matching_idx])
                # ========================================
                loss = -torch.log(q_m[idx_b] + 1e-8)

                total_loss += loss
                num_matches += 1

        if num_matches > 0:
            return total_loss / num_matches
        else:
            return torch.tensor(0.0, device=kpts_a.device)

    def _reliable_loss(
        self,
        outputs_a: Dict,
        outputs_b: Dict,
        H_ab: torch.Tensor,
        depth_a: torch.Tensor = None,
        R_ab: torch.Tensor = None,
        t_ab: torch.Tensor = None
    ) -> torch.Tensor:
        """
        Reliable Loss (è«–æ–‡ Section V-D, å¼12-13)

        ãƒãƒƒãƒãƒ³ã‚°å¯¾å¿œç‚¹ã§ã®è¨˜è¿°å­ã®ä¿¡é ¼æ€§ã«åŸºã¥ã„ã¦ã‚¹ã‚³ã‚¢ã‚’èª¿æ•´:
        - å¯¾å¿œç‚¹ã®è¨˜è¿°å­ãŒæ˜ç¢ºã«ãƒãƒƒãƒã™ã‚‹ â†’ é«˜ä¿¡é ¼æ€§ â†’ é«˜ã‚¹ã‚³ã‚¢ç¶­æŒ
        - å¯¾å¿œç‚¹ã®è¨˜è¿°å­ãŒæ›–æ˜§ â†’ ä½ä¿¡é ¼æ€§ â†’ ã‚¹ã‚³ã‚¢ã‚’ä¸‹ã’ã‚‹

        æ•°å¼:
        r(pA, I_B) = softmax(sim(dA, D_B) / t_rel)[idx_b]  (å¼12, å¯¾å¿œç‚¹ã®index)
        L_re = (1 / ÅœA) * Î£ (1 - r(pA, I_B)) * sA           (å¼13)

        å…¥åŠ›:
            outputs_a, outputs_b: ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆãƒ»è¨˜è¿°å­ãƒ»ã‚¹ã‚³ã‚¢æƒ…å ±
            H_ab: Homographyè¡Œåˆ—
            depth_a, R_ab, t_ab: Perspective projectionç”¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        å‡ºåŠ›:
            loss: scalar
        """

        kpts_a = outputs_a['keypoints']      # (B, N_a, 2)
        desc_a = outputs_a['descriptors']    # (B, N_a, dim)
        scores_a = outputs_a['scores']       # (B, N_a)

        kpts_b = outputs_b['keypoints']      # (B, N_b, 2)
        desc_b = outputs_b['descriptors']    # (B, N_b, dim)

        B = kpts_a.shape[0]

        total_loss = 0.0

        for b in range(B):
            SA = scores_a[b]  # (N_a,)
            DB = desc_b[b]    # (N_b, dim)

            # ========================================
            # Step 1: å¹¾ä½•çš„å¯¾å¿œã‹ã‚‰ãƒãƒƒãƒãƒ³ã‚°ãƒšã‚¢ã‚’è¦‹ã¤ã‘ã‚‹
            # ========================================
            kpts_a_warped = self._warp_keypoints(
                kpts_a[b],
                H_ab[b] if H_ab is not None else None,
                depth_a[b] if depth_a is not None else None,
                R_ab[b] if R_ab is not None else None,
                t_ab[b] if t_ab is not None else None
            )

            matches_ab = self._find_nearest_neighbors(
                kpts_a_warped,
                kpts_b[b],
                distance_threshold=5.0
            )

            if len(matches_ab) == 0:
                continue

            # ========================================
            # Step 2: ãƒãƒƒãƒãƒ³ã‚°ãƒšã‚¢ã”ã¨ã«Reliabilityè¨ˆç®—
            # ========================================
            weighted_loss = 0.0
            score_sum = 0.0

            for idx_a, idx_b in matches_ab:
                dA = desc_a[b, idx_a]   # (dim,)
                sA = SA[idx_a]          # scalar

                # Matching similarity vector (å¼9)
                sim = torch.matmul(DB, dA)  # (N_b,)

                # Reliability: å¯¾å¿œç‚¹ã§ã®softmaxå€¤ (å¼12)
                r_vec = F.softmax(sim / self.trel, dim=0)  # (N_b,)
                r = r_vec[idx_b]  # å¯¾å¿œç‚¹ã®reliability (scalar)

                # é‡ã¿ä»˜ãloss (å¼13)
                weighted_loss += (1.0 - r) * sA
                score_sum += sA

            # æ­£è¦åŒ– (å¼13: 1/ÅœA)
            if score_sum > 0:
                loss_b = weighted_loss / (score_sum + 1e-8)
                total_loss += loss_b

        return total_loss / B

    def _warp_keypoints(
        self,
        keypoints: torch.Tensor,
        H: torch.Tensor = None,
        depth: torch.Tensor = None,
        R: torch.Tensor = None,
        t: torch.Tensor = None
    ) -> torch.Tensor:
        """
        ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ¯ãƒ¼ãƒ—

        Homography mode: Hé©ç”¨
        Perspective mode: 3D projection

        å…¥åŠ›:
            keypoints: (N, 2) - [x, y]
            H: (3, 3) - Homography matrix
            (or depth, R, t for perspective)

        å‡ºåŠ›:
            warped: (N, 2) - [x', y']
        """

        if H is not None:
            # Homographyå¤‰æ›
            kpts_homo = torch.cat([
                keypoints,
                torch.ones(keypoints.shape[0], 1, device=keypoints.device)
            ], dim=1)  # (N, 3)

            kpts_warped_homo = torch.matmul(kpts_homo, H.T)  # (N, 3)

            # æ­£è¦åŒ–
            kpts_warped = kpts_warped_homo[:, :2] / kpts_warped_homo[:, 2:3]

            return kpts_warped

        else:
            # Perspectiveå¤‰æ› (ç°¡ç•¥ç‰ˆ)
            # å®Ÿè£…ã§ã¯ depth map ã¨ R, t ã‚’ä½¿ç”¨
            return keypoints  # ç°¡ç•¥åŒ–ã®ãŸã‚

    def _find_nearest_neighbors(
        self,
        kpts_src: torch.Tensor,
        kpts_tgt: torch.Tensor,
        distance_threshold: float = 5.0
    ) -> List[Tuple[int, int]]:
        """
        æœ€è¿‘å‚ãƒãƒƒãƒãƒ³ã‚°

        å…¥åŠ›:
            kpts_src: (N_src, 2)
            kpts_tgt: (N_tgt, 2)
            distance_threshold: float - ãƒ”ã‚¯ã‚»ãƒ«

        å‡ºåŠ›:
            matches: List[(idx_src, idx_tgt)]
        """

        # è·é›¢è¡Œåˆ—
        dist_matrix = torch.cdist(kpts_src, kpts_tgt)  # (N_src, N_tgt)

        # æœ€è¿‘å‚
        min_dists, min_indices = dist_matrix.min(dim=1)  # (N_src,)

        # é–¾å€¤é©ç”¨
        valid_mask = min_dists < distance_threshold

        matches = []
        for i in range(kpts_src.shape[0]):
            if valid_mask[i]:
                matches.append((i, min_indices[i].item()))

        return matches

# ============================================
# ä½¿ç”¨ä¾‹
# ============================================

def example_loss():
    """æå¤±è¨ˆç®—ã®ä½¿ç”¨ä¾‹"""

    # æå¤±ãƒ©ãƒƒãƒ‘ãƒ¼
    loss_wrapper = ALIKEDLossWrapper(
        w_rp=1.0,
        w_pk=0.5,
        w_ds=5.0,
        w_re=1.0
    )

    # ãƒ€ãƒŸãƒ¼å‡ºåŠ›
    outputs_a = {
        'keypoints': torch.rand(2, 500, 2) * 100,
        'descriptors': F.normalize(torch.randn(2, 500, 128), p=2, dim=-1),
        'scores': torch.rand(2, 500),
        'score_map': torch.rand(2, 1, 160, 120)
    }

    outputs_b = {
        'keypoints': torch.rand(2, 500, 2) * 100,
        'descriptors': F.normalize(torch.randn(2, 500, 128), p=2, dim=-1),
        'scores': torch.rand(2, 500),
        'score_map': torch.rand(2, 1, 160, 120)
    }

    # Homography
    H_ab = torch.eye(3).unsqueeze(0).repeat(2, 1, 1)
    H_ab[:, 0, 2] = 10  # Translation

    # æå¤±è¨ˆç®—
    losses = loss_wrapper(outputs_a, outputs_b, H_ab)

    print("Losses:")
    for k, v in losses.items():
        print(f"  {k}: {v.item():.4f}")

if __name__ == "__main__":
    example_loss()
