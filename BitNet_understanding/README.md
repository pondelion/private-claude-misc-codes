# BitNet Understanding - ç°¡ç•¥åŒ–ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰é›†

BitNet b1.58 ãŠã‚ˆã³ BitNet Distillation (BitDistill) ã®ç†è§£ã‚’ç›®çš„ã¨ã—ãŸç°¡ç•¥åŒ–ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰é›†ã§ã™ã€‚

è«–æ–‡: [BitNet Distillation](https://arxiv.org/abs/2510.13998) (arXiv 2025, Microsoft Research)
å…¬å¼å®Ÿè£…: [github.com/microsoft/BitNet](https://github.com/microsoft/BitNet)

## ğŸ“‹ ç›®æ¬¡

- [æ¦‚è¦](#æ¦‚è¦)
- [ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å…¨ä½“åƒ](#ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å…¨ä½“åƒ)
- [ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ](#ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ)
- [BitNetã®ä¸»è¦ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³](#bitnetã®ä¸»è¦ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³)
- [å¾“æ¥ã®é‡å­åŒ–æ‰‹æ³•ã¨ã®æ¯”è¼ƒ](#å¾“æ¥ã®é‡å­åŒ–æ‰‹æ³•ã¨ã®æ¯”è¼ƒ)
- [å‡¦ç†ãƒ•ãƒ­ãƒ¼è©³ç´°](#å‡¦ç†ãƒ•ãƒ­ãƒ¼è©³ç´°)
- [BitDistillè’¸ç•™ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³](#bitdistillè’¸ç•™ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³)
- [å…¬å¼å®Ÿè£…ã®æ§‹æˆ](#å…¬å¼å®Ÿè£…ã®æ§‹æˆ)
- [å½¢çŠ¶ã‚¬ã‚¤ãƒ‰](#å½¢çŠ¶ã‚¬ã‚¤ãƒ‰)
- [Qwen3-0.6Bã¸ã®é©ç”¨ã‚µãƒ³ãƒ—ãƒ«](#qwen3-06bã¸ã®é©ç”¨ã‚µãƒ³ãƒ—ãƒ«)
- [FAQ](#faq)
- [å‚è€ƒæ–‡çŒ®](#å‚è€ƒæ–‡çŒ®)

---

## æ¦‚è¦

**BitNetã®ç‰¹å¾´:**
- **1.58-bité‡å­åŒ–**: é‡ã¿ã‚’ä¸‰å€¤ {-1, 0, 1} ã«é‡å­åŒ–ï¼ˆç†è«–çš„ã« logâ‚‚(3) â‰ˆ 1.58 bitsï¼‰
- **é«˜é€Ÿæ¨è«–**: CPUä¸Šã§æœ€å¤§2.65å€ã®é«˜é€ŸåŒ–ã€GPUã§æœ€å¤§3.27å€
- **çœãƒ¡ãƒ¢ãƒª**: æœ€å¤§10å€ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ï¼ˆFP16æ¯”ï¼‰
- **ãƒ­ã‚¹ãƒ¬ã‚¹æ¨è«–**: 1-bit LLMã«ãŠã„ã¦ç²¾åº¦åŠ£åŒ–ãªã—ã®æ¨è«–ã‚«ãƒ¼ãƒãƒ«
- **è’¸ç•™å¯¾å¿œ**: BitDistillã«ã‚ˆã‚Šæ—¢å­˜FP16ãƒ¢ãƒ‡ãƒ«ã‚’1.58-bitã«å¤‰æ›å¯èƒ½

**ã‚¿ã‚¹ã‚¯:**
- LLMæ¨è«–ã®é«˜é€ŸåŒ–ãƒ»çœãƒ¡ãƒ¢ãƒªåŒ–
- Full-precision â†’ 1.58-bit ã®çŸ¥è­˜è’¸ç•™
- ã‚¿ã‚¹ã‚¯ç‰¹åŒ–å‹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

**æ€§èƒ½** (BitDistill, Qwen3-0.6B, MNLI):
| Method | MNLI Acc | QNLI Acc | SST-2 Acc | ãƒ¡ãƒ¢ãƒª | æ¨è«–é€Ÿåº¦ |
|--------|----------|----------|-----------|--------|----------|
| FP16-SFT | 88.01 | 93.72 | 94.21 | 1.20 GB | 427 tok/s |
| BitNet-SFT (ç›´æ¥QAT) | 74.09 | 78.32 | 79.92 | 0.11 GB | 1,135 tok/s |
| **BitDistill** | **88.17** | **93.66** | **94.30** | **0.11 GB** | **1,135 tok/s** |

---

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å…¨ä½“åƒ

### BitNet b1.58 é‡å­åŒ–ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
å…¥åŠ›: Full-precision Transformer (ä¾‹: Qwen3, Llama3)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Weight Quantization (1.58-bit)                     â”‚
â”‚    W_FP16 â†’ {-1, 0, 1} (ä¸‰å€¤)                        â”‚
â”‚    Î” = mean(|W|)                                      â”‚
â”‚    Q(W) = Î” Ã— RoundClip(W / (Î”+Îµ), -1, 1)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Activation Quantization (8-bit)                    â”‚
â”‚    X_FP16 â†’ INT8                                      â”‚
â”‚    Î³ = max(|X|)                                       â”‚
â”‚    Q(X) = (Î³/127) Ã— RoundClip(127X/Î³, -128, 127)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. SubLN (Sub-Layer Normalization) [BitDistillè¿½åŠ ]   â”‚
â”‚    MHSAå‡ºåŠ›æŠ•å½±å‰ + FFNå‡ºåŠ›æŠ•å½±å‰ã«LayerNormæŒ¿å…¥      â”‚
â”‚    â†’ æ´»æ€§åŒ–ã®åˆ†æ•£ã‚’å®‰å®šåŒ–                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Transformer Layer (lç•ªç›®)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Multi-Head Self-Attention                  â”‚      â”‚
â”‚  â”‚  Q,K,V = X Ã— W_Q, X Ã— W_K, X Ã— W_V       â”‚      â”‚
â”‚  â”‚  heads = Softmax(QKáµ€/âˆšd) Ã— V              â”‚      â”‚
â”‚  â”‚  Y = X + SubLN(Concat(heads)) Ã— W_out     â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                    â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Feed-Forward Network (SwiGLU)              â”‚      â”‚
â”‚  â”‚  gate = Ïƒ(Y Ã— W_gate)                     â”‚      â”‚
â”‚  â”‚  up = Y Ã— W_up                            â”‚      â”‚
â”‚  â”‚  X' = Y + SubLN(gate âŠ™ up) Ã— W_down      â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                    â†“                                  â”‚
â”‚  â€» W_Q, W_K, W_V, W_out, W_gate, W_up, W_down      â”‚
â”‚    å…¨ã¦1.58-bité‡å­åŒ–æ¸ˆã¿                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“ (Ã— L layers)
å‡ºåŠ›: Next token prediction
```

### BitDistill è’¸ç•™ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 1: ãƒ¢ãƒ‡ãƒªãƒ³ã‚°æ”¹è‰¯ (SubLNæŒ¿å…¥)                         â”‚
â”‚    FP16 Pre-trained Model                                  â”‚
â”‚         â†“                                                  â”‚
â”‚    SubLNã‚’MHSAå‡ºåŠ›æŠ•å½±å‰ + FFNå‡ºåŠ›æŠ•å½±å‰ã«æŒ¿å…¥               â”‚
â”‚    â†’ æ´»æ€§åŒ–ã®åˆ†æ•£çˆ†ç™ºã‚’é˜²æ­¢                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 2: ç¶™ç¶šäº‹å‰å­¦ç¿’ (Continual Pre-Training)              â”‚
â”‚    10B tokens (FALCON corpus) ã§è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°             â”‚
â”‚    â†’ é‡ã¿åˆ†å¸ƒã‚’ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã‹ã‚‰BitNetå‘ãåˆ†å¸ƒã«å¤‰æ›            â”‚
â”‚    â†’ {0,Â±1}ã®é·ç§»å¢ƒç•Œä»˜è¿‘ã«é‡ã¿ã‚’é›†ä¸­                       â”‚
â”‚    â€» ãƒ•ãƒ«ã‚¹ã‚¯ãƒ©ãƒƒãƒå­¦ç¿’ã®4T tokensã«å¯¾ã—ã€ã»ã¼ç„¡è¦–ã§ãã‚‹ã‚³ã‚¹ãƒˆ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 3: è’¸ç•™ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ Teacher (FP16)   â”‚    â”‚ Student (1.58-bit)â”‚             â”‚
â”‚  â”‚ Fine-tuned model â”‚    â”‚ Quantized model   â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚           â”‚                       â”‚                        â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                   â†“                                        â”‚
â”‚  L = L_CE + Î»Ã—L_LD + Î³Ã—L_AD                              â”‚
â”‚    L_CE : Cross-Entropyæå¤±                                â”‚
â”‚    L_LD : Logitsè’¸ç•™ (KL divergence, Ï„=5.0)              â”‚
â”‚    L_AD : Attentionè’¸ç•™ (MiniLMæ–¹å¼, å˜ä¸€ãƒ¬ã‚¤ãƒ¤ãƒ¼)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
å‡ºåŠ›: 1.58-bité‡å­åŒ–æ¸ˆã¿ã‚¿ã‚¹ã‚¯ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«
```

---

## ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

### âš ï¸ å…¬å¼å®Ÿè£…ã¨ã®é–¢ä¿‚ãƒ»å„ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½ç½®ã¥ã‘

BitNetã®å…¬å¼ãƒªãƒã‚¸ãƒˆãƒªã¯**æ¨è«–ç”¨ã®C++/CUDAã‚«ãƒ¼ãƒãƒ«**ã®ã¿ã‚’å…¬é–‹ã—ã¦ãŠã‚Šã€**å­¦ç¿’ï¼ˆè’¸ç•™ï¼‰ç”¨ã®ã‚³ãƒ¼ãƒ‰ã¯éå…¬é–‹**ã§ã™ã€‚
æœ¬ãƒªãƒã‚¸ãƒˆãƒªã®å„ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«åˆ†é¡ã•ã‚Œã¾ã™:

```
å…¬å¼ãŒå…¬é–‹ã—ã¦ã„ã‚‹ã‚‚ã®:
  æ¨è«–ã‚«ãƒ¼ãƒãƒ« (C++/CUDA) â†’ é«˜é€Ÿãª1.58-bitæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³

å…¬å¼ãŒå…¬é–‹ã—ã¦ã„ãªã„ã‚‚ã®:
  å­¦ç¿’ã‚³ãƒ¼ãƒ‰ (è’¸ç•™ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³) â†’ è«–æ–‡ã®è¨˜è¿°ã®ã¿

æœ¬ãƒªãƒã‚¸ãƒˆãƒªã§ã®å¯¾å¿œ:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ãƒ•ã‚¡ã‚¤ãƒ«                  â”‚ ãƒ•ã‚§ãƒ¼ã‚º    â”‚ èª¬æ˜                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ bitnet_quantization.py   â”‚ å­¦ç¿’       â”‚ ä¸‰å€¤é‡å­åŒ–ãƒ»SubLNç­‰ã‚’PyTorchã§å®Ÿè£…  â”‚
â”‚ bitdistill_pipeline.py   â”‚ å­¦ç¿’       â”‚ è’¸ç•™3æ®µéšãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (è«–æ–‡ã‹ã‚‰å†å®Ÿè£…)â”‚
â”‚ distillation_losses.py   â”‚ å­¦ç¿’       â”‚ è’¸ç•™æå¤±é–¢æ•° (è«–æ–‡ã‹ã‚‰å†å®Ÿè£…)        â”‚
â”‚ qwen3_finetune_example.pyâ”‚ å­¦ç¿’       â”‚ Qwen3-0.6Bã¸ã®é©ç”¨ã‚µãƒ³ãƒ—ãƒ«          â”‚
â”‚ inference_cpp_kernel.py  â”‚ æ¨è«–       â”‚ å…¬å¼C++ã‚«ãƒ¼ãƒãƒ«ã®Pythonç–‘ä¼¼ã‚³ãƒ¼ãƒ‰     â”‚
â”‚ cpp_implementation.cpp   â”‚ æ¨è«–       â”‚ å…¬å¼C++ã‚«ãƒ¼ãƒãƒ«ã®ç°¡ç•¥åŒ–C++å®Ÿè£…        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**é‡è¦ãªé•ã„:**
- **å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚º** (Python): PyTorchã®FP32æ¼”ç®—ã§ä¸‰å€¤é‡å­åŒ–ã‚’**ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³**ã—ã¦å­¦ç¿’ã™ã‚‹ã€‚
  å®Ÿéš›ã«ã¯ `float32` ã®ã¾ã¾è¨ˆç®—ã—ã€STE (Straight-Through Estimator) ã§å‹¾é…ã‚’é€šã™ã€‚
  ãƒ“ãƒƒãƒˆæ¼”ç®—ã«ã‚ˆã‚‹é«˜é€ŸåŒ–ã¯è¡Œã‚ãªã„ã€‚ã“ã‚Œã¯å…¬å¼ã®å­¦ç¿’ã‚³ãƒ¼ãƒ‰ãŒéå…¬é–‹ã®ãŸã‚ã€è«–æ–‡ã®è¨˜è¿°ã‹ã‚‰å†å®Ÿè£…ã—ãŸã‚‚ã®ã€‚
- **æ¨è«–ãƒ•ã‚§ãƒ¼ã‚º** (C++/CUDA): å­¦ç¿’æ¸ˆã¿ã®ä¸‰å€¤é‡ã¿ã‚’2-bitã«ãƒ‘ãƒƒã‚­ãƒ³ã‚°ã—ã€SIMDå‘½ä»¤ã§
  é«˜é€Ÿã«åŠ æ¸›ç®—ã‚’è¡Œã†ã€‚ã“ã¡ã‚‰ãŒå…¬å¼ãƒªãƒã‚¸ãƒˆãƒªã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹éƒ¨åˆ†ã€‚
  `inference_cpp_kernel.py` ã¯ã“ã®å‹•ä½œã‚’Pythonã§è¡¨ç¾ã—ãŸç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ã€
  `cpp_implementation.cpp` ã¯ç°¡ç•¥åŒ–ã—ãŸC++å®Ÿè£…ã€‚

ã¤ã¾ã‚Š `qwen3_finetune_example.py` ã¯ã€Œå…¬å¼C++ã®æ›¸ãæ›ãˆã€ã§ã¯ãªãã€
**å…¬å¼ãŒå…¬é–‹ã—ã¦ã„ãªã„å­¦ç¿’éƒ¨åˆ†ã‚’è«–æ–‡ã‹ã‚‰å†å®Ÿè£…ã—ãŸã‚‚ã®**ã§ã™ã€‚

---

### 1. [bitnet_quantization.py](bitnet_quantization.py)
**1.58-bité‡å­åŒ–ã®æ ¸å¿ƒãƒ­ã‚¸ãƒƒã‚¯ï¼ˆPythonå®Ÿè£…ï¼‰** [å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚º]

é‡ã¿ã®ä¸‰å€¤é‡å­åŒ–ã€æ´»æ€§åŒ–ã®8-bité‡å­åŒ–ã€STE (Straight-Through Estimator) ã«ã‚ˆã‚‹å‹¾é…è¿‘ä¼¼ã‚’å®Ÿè£…:

```python
class BitLinear(nn.Module):
    """
    1.58-bitç·šå½¢å±¤

    å¾“æ¥æ‰‹æ³•:
      - INT4/INT8é‡å­åŒ–: ç²¾åº¦ã¯ä¿æŒã™ã‚‹ãŒãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã¯é™å®šçš„
      - GPTQ/AWQ: Post-trainingé‡å­åŒ–ã§ç²¾åº¦åŠ£åŒ–

    BitNet b1.58:
      - é‡ã¿: {-1, 0, 1} ã®ä¸‰å€¤ã®ã¿
      - ä¹—ç®—ãŒä¸è¦ â†’ åŠ æ¸›ç®—ã®ã¿ã§è¨ˆç®—å¯èƒ½
      - ç†è«–çš„ã«1.58 bits/weight

    æ•°å­¦çš„è¡¨ç¾:
      Î” = mean(|W|)
      Q_w(W) = Î” Ã— RoundClip(W/(Î”+Îµ), -1, 1)
      Q_a(X) = (Î³/127) Ã— RoundClip(127Â·X/(Î³+Îµ), -128, 127)
      Y = Q_a(X) @ Q_w(W)áµ€
    """
    def forward(self, x):
        # é‡ã¿é‡å­åŒ– (1.58-bit)
        delta = self.weight.abs().mean()
        w_quant = delta * round_clip(self.weight / (delta + eps), -1, 1)

        # æ´»æ€§åŒ–é‡å­åŒ– (8-bit)
        gamma = x.abs().max(dim=-1, keepdim=True).values
        x_quant = (gamma / 127) * round_clip(127 * x / (gamma + eps), -128, 127)

        return F.linear(x_quant, w_quant)
```

### 2. [bitdistill_pipeline.py](bitdistill_pipeline.py)
**BitDistill 3æ®µéšè’¸ç•™ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³** [å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚º / è«–æ–‡ã‹ã‚‰å†å®Ÿè£…]

SubLNæŒ¿å…¥ã€ç¶™ç¶šäº‹å‰å­¦ç¿’ã€è’¸ç•™ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®å…¨ä½“ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè£…:

```python
class BitDistillPipeline:
    """
    3æ®µéšãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³:
      Stage 1: SubLNæŒ¿å…¥ â†’ æ´»æ€§åŒ–ã®åˆ†æ•£å®‰å®šåŒ–
      Stage 2: ç¶™ç¶šäº‹å‰å­¦ç¿’ (10B tokens) â†’ é‡ã¿åˆ†å¸ƒå¤‰æ›
      Stage 3: è’¸ç•™ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚° â†’ ã‚¿ã‚¹ã‚¯ç‰¹åŒ–

    åŠ¹æœ:
      - ç›´æ¥QAT: 74.09% (MNLI) â†’ BitDistill: 88.17%
      - FP16ã®88.01%ã¨ã»ã¼åŒç­‰ã®ç²¾åº¦ã‚’1.58-bitã§é”æˆ
    """
    def distill_finetune(self, teacher, student, dataloader):
        for batch in dataloader:
            # Teacher forward (FP16, frozen)
            with torch.no_grad():
                t_logits, t_attns = teacher(batch)

            # Student forward (1.58-bit)
            s_logits, s_attns = student(batch)

            # æå¤±è¨ˆç®—
            loss_ce = cross_entropy(s_logits, batch['labels'])
            loss_ld = kl_divergence(t_logits/tau, s_logits/tau)
            loss_ad = attention_distill(t_attns, s_attns)

            loss = loss_ce + lambda_ * loss_ld + gamma_ * loss_ad
            loss.backward()
```

### 3. [distillation_losses.py](distillation_losses.py)
**è’¸ç•™æå¤±é–¢æ•°ã®å®Ÿè£…** [å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚º / è«–æ–‡ã‹ã‚‰å†å®Ÿè£…]

Logitsè’¸ç•™ï¼ˆKL divergenceï¼‰ã¨Multi-Head Attentionè’¸ç•™ï¼ˆMiniLMæ–¹å¼ï¼‰:

```python
class DistillationLoss(nn.Module):
    """
    2ç¨®é¡ã®è’¸ç•™æå¤±:

    1. Logitsè’¸ç•™:
       L_LD = KL(P_teacher(Ï„) || P_student(Ï„))

    2. Attentionè’¸ç•™ (MiniLMæ–¹å¼):
       - Q, K, V ãã‚Œãã‚Œã®é–¢ä¿‚è¡Œåˆ—ã‚’è’¸ç•™
       - å˜ä¸€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã¿ (å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚ˆã‚ŠåŠ¹æœçš„)
       R = Softmax(AÂ·Aáµ€/âˆšd)
       L_AD = KL(R_teacher || R_student)
    """
```

### 4. [inference_cpp_kernel.py](inference_cpp_kernel.py)
**C++æ¨è«–ã‚«ãƒ¼ãƒãƒ«ã®ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ï¼ˆPythonè¡¨ç¾ï¼‰** [æ¨è«–ãƒ•ã‚§ãƒ¼ã‚º / å…¬å¼C++ã®Pythonç–‘ä¼¼ã‚³ãƒ¼ãƒ‰]

å…¬å¼å®Ÿè£… (`ggml-bitnet-mad.cpp`) ã®SIMDæœ€é©åŒ–ã‚«ãƒ¼ãƒãƒ«ã®å‹•ä½œã‚’Pythonã§è¡¨ç¾ã€‚
å®Ÿç”¨é€Ÿåº¦ã§ã¯ãªã„ãŒã€ãƒ‘ãƒƒã‚­ãƒ³ã‚°ãƒ»å±•é–‹ãƒ»SIMDæ¼”ç®—ã®ä»•çµ„ã¿ã‚’ç†è§£ã™ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ‰:

```python
def ternary_matmul_simd(weights_packed, activations):
    """
    C++å®Ÿè£…ã®Pythonç–‘ä¼¼ã‚³ãƒ¼ãƒ‰

    å…¬å¼å®Ÿè£… (ggml-bitnet-mad.cpp):
      - 4ã¤ã®ä¸‰å€¤ã‚’1ãƒã‚¤ãƒˆã«ãƒ‘ãƒƒã‚¯ (2 bits Ã— 4)
      - AVX2/NEONã§é«˜é€Ÿå±•é–‹ãƒ»ä¹—ç®—
      - ä¹—ç®—ã‚’åŠ æ¸›ç®—ã§ä»£æ›¿
    """
```

### 5. [cpp_implementation.cpp](cpp_implementation.cpp)
**C++ã§ã®1.58-bité‡å­åŒ–ãƒ»æ¨è«–ã®å®Ÿè£…ä¾‹** [æ¨è«–ãƒ•ã‚§ãƒ¼ã‚º / å…¬å¼C++ã®ç°¡ç•¥åŒ–ç‰ˆ]

å…¬å¼å®Ÿè£… (`ggml-bitnet-mad.cpp`) ã‚’ç°¡ç•¥åŒ–ã—ãŸæ•™è‚²ç”¨C++ã‚³ãƒ¼ãƒ‰ã€‚
AVX2 SIMD intrinsics ã‚’ç”¨ã„ãŸé«˜é€Ÿæ¨è«–ã‚«ãƒ¼ãƒãƒ«ã®å‹•ä½œã‚’ç¤ºã™ã€‚
ãƒ•ã‚¡ã‚¤ãƒ«å…ˆé ­ã«ãƒ“ãƒ«ãƒ‰æ–¹æ³•ãƒ»å®Ÿè¡Œæ–¹æ³•ãƒ»ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚’è¨˜è¼‰:

```cpp
// ä¸‰å€¤é‡å­åŒ– + ãƒ‘ãƒƒã‚­ãƒ³ã‚°
void quantize_ternary(const float* src, uint8_t* dst, float* scale, int n);

// AVX2ã«ã‚ˆã‚‹é«˜é€Ÿè¡Œåˆ—ãƒ™ã‚¯ãƒˆãƒ«ç©
void ternary_matvec_avx2(const uint8_t* weights, const int8_t* input,
                          float* output, float scale, int rows, int cols);
```

### 6. [qwen3_finetune_example.py](qwen3_finetune_example.py)
**Qwen3-0.6Bã«å¯¾ã™ã‚‹BitDistillé©ç”¨ã‚µãƒ³ãƒ—ãƒ«** [å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚º / è«–æ–‡ã‹ã‚‰å†å®Ÿè£…]

è«–æ–‡ã®BitDistillæ‰‹æ³•ã‚’Qwen3-0.6Bã«é©ç”¨ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã€‚
å…¬å¼C++ã‚³ãƒ¼ãƒ‰ã®æ›¸ãæ›ãˆã§ã¯ãªãã€**å…¬å¼ãŒéå…¬é–‹ã®å­¦ç¿’éƒ¨åˆ†ã‚’è«–æ–‡ã‹ã‚‰å†å®Ÿè£…**ã—ãŸã‚‚ã®ã€‚
PyTorchã®FP32æ¼”ç®—ã§ä¸‰å€¤é‡å­åŒ–ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ã¦å­¦ç¿’ã™ã‚‹ã€‚
`transformers` ãŒç„¡ã„ç’°å¢ƒã§ã‚‚ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«ã§ãƒ‡ãƒ¢å¯èƒ½:

```python
# Qwen3-0.6Bã‚’1.58-bit BitNetã«å¤‰æ›
model = convert_to_bitnet(Qwen3ForCausalLM.from_pretrained("Qwen/Qwen3-0.6B"))

# å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§è’¸ç•™ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
trainer = BitDistillTrainer(teacher=teacher_model, student=model, ...)
trainer.train()
```

---

## BitNetã®ä¸»è¦ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³

### ğŸ”‘ **ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³1: 1.58-bitä¸‰å€¤é‡å­åŒ–**

```python
def ternary_quantize(weight):
    """
    å¾“æ¥ã®é‡å­åŒ–:
      INT8: 256ãƒ¬ãƒ™ãƒ« (8 bits/weight)
      INT4: 16ãƒ¬ãƒ™ãƒ« (4 bits/weight)
      â†’ ä¹—ç®—ãŒå¿…è¦

    BitNet b1.58:
      {-1, 0, 1}: 3ãƒ¬ãƒ™ãƒ« (1.58 bits/weight)
      â†’ ä¹—ç®—ãŒåŠ æ¸›ç®—ã«å¤‰æ›å¯èƒ½!
        y += w * x  â†’  if w==1: y+=x  elif w==-1: y-=x  else: pass

    åŠ¹æœ:
      - ãƒ¡ãƒ¢ãƒª: FP16ã®ç´„1/10
      - è¨ˆç®—: ä¹—ç®—ä¸è¦ã§å¤§å¹…é«˜é€ŸåŒ–
      - ã‚¨ãƒãƒ«ã‚®ãƒ¼: 55-82%å‰Šæ¸›
    """
    delta = weight.abs().mean()  # per-tensor absmean
    w_normalized = weight / (delta + 1e-8)
    w_ternary = torch.clamp(torch.round(w_normalized), -1, 1)
    return w_ternary, delta
```

### ğŸ”‘ **ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³2: SubLN (Sub-Layer Normalization)**

```python
class TransformerLayerWithSubLN(nn.Module):
    """
    å•é¡Œ:
      1.58-bité‡å­åŒ–ã§ã¯æ´»æ€§åŒ–ã®åˆ†æ•£ãŒçˆ†ç™ºçš„ã«å¢—å¤§
      â†’ å­¦ç¿’ãŒä¸å®‰å®šåŒ–ã€åæŸå›°é›£

    è§£æ±ºç­– (SubLN):
      MHSAå‡ºåŠ›æŠ•å½±å‰ + FFNå‡ºåŠ›æŠ•å½±å‰ã«LayerNormã‚’è¿½åŠ 
      â†’ é‡å­åŒ–å±¤ã«å…¥ã‚‹å‰ã«åˆ†æ•£ã‚’å®‰å®šåŒ–

    é…ç½®:
      Y = X + SubLN(Concat(heads)) Ã— W_out    [MHSA]
      X' = Y + SubLN(gate âŠ™ up) Ã— W_down      [FFN]

    åŠ¹æœ:
      - SubLNãªã—: 74.09% â†’ SubLNã‚ã‚Š: 76.30% (MNLI)
      - å­¦ç¿’ã®å®‰å®šåŒ–ãƒ»åæŸæ”¹å–„
    """
    def forward(self, x):
        # MHSA with SubLN
        attn_out = self.multi_head_attention(x)
        attn_out = self.sub_ln_attn(attn_out)  # â† SubLNè¿½åŠ 
        y = x + self.out_proj(attn_out)

        # FFN with SubLN
        gate = torch.sigmoid(self.w_gate(y))
        up = self.w_up(y)
        ffn_out = self.sub_ln_ffn(gate * up)    # â† SubLNè¿½åŠ 
        return y + self.w_down(ffn_out)
```

### ğŸ”‘ **ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³3: ç¶™ç¶šäº‹å‰å­¦ç¿’ã«ã‚ˆã‚‹é‡ã¿åˆ†å¸ƒå¤‰æ›**

```python
def continual_pretraining(model, corpus, num_tokens=10_000_000_000):
    """
    å•é¡Œ (ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£å•é¡Œ):
      FP16 â†’ 1.58-bit ã®ç›´æ¥QATã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãããªã‚‹ã»ã©
      ç²¾åº¦ã‚®ãƒ£ãƒƒãƒ—ãŒæ‹¡å¤§:
        0.6B: 13.9ãƒã‚¤ãƒ³ãƒˆå·®
        1.7B: 14.3ãƒã‚¤ãƒ³ãƒˆå·®
        4B:   15.3ãƒã‚¤ãƒ³ãƒˆå·®

    åŸå› :
      FP16ã®é‡ã¿åˆ†å¸ƒ â‰ˆ ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ
      â†’ ä¸‰å€¤é‡å­åŒ–ã®é·ç§»å¢ƒç•Œ (0/Â±1) ã‹ã‚‰é›¢ã‚ŒãŸé‡ã¿ãŒå¤šã„
      â†’ å°ã•ãªå‹¾é…ã§ã¯é‡å­åŒ–å€¤ãŒå¤‰åŒ–ã—ãªã„

    è§£æ±ºç­–:
      10B tokensã®ç¶™ç¶šäº‹å‰å­¦ç¿’ã§é‡ã¿åˆ†å¸ƒã‚’å¤‰æ›
      â†’ é·ç§»å¢ƒç•Œä»˜è¿‘ (0â†”-1, 0â†”1) ã«é‡ã¿ã‚’é›†ä¸­
      â†’ å‹¾é…æ›´æ–°ã§é‡å­åŒ–å€¤ãŒã‚·ãƒ•ãƒˆã—ã‚„ã™ããªã‚‹

    åŠ¹æœ:
      - ã‚³ã‚¹ãƒˆ: ãƒ•ãƒ«ã‚¹ã‚¯ãƒ©ãƒƒãƒã®4T tokensã®0.25% (10B tokens)
      - SubLNã®ã¿: 76.30% â†’ +CT: 86.73% (MNLI, +10.4ãƒã‚¤ãƒ³ãƒˆ)
    """
    optimizer = AdamW(model.parameters(), lr=1e-4)
    for batch in corpus:
        logits = model(batch['input_ids'])
        loss = cross_entropy(logits, batch['labels'])  # æ¨™æº–è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°
        loss.backward()
        optimizer.step()
```

### ğŸ”‘ **ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³4: Multi-Head Attentionè’¸ç•™ï¼ˆMiniLMæ–¹å¼ï¼‰**

```python
def attention_distillation(teacher_attn, student_attn, layer_idx=-1):
    """
    å¾“æ¥ã®è’¸ç•™:
      - Logitsã®ã¿è’¸ç•™ â†’ æ§‹é€ çš„çŸ¥è­˜ãŒæ¬ è½
      - å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼è’¸ç•™ â†’ å­¦ç”Ÿãƒ¢ãƒ‡ãƒ«ã®è‡ªç”±åº¦ãŒä½ä¸‹

    BitDistillã®å·¥å¤«:
      - Q, K, V ãã‚Œãã‚Œã®é–¢ä¿‚è¡Œåˆ—ã‚’è’¸ç•™
      - å˜ä¸€ãƒ¬ã‚¤ãƒ¤ãƒ¼(å¾ŒåŠ)ã®ã¿ â†’ æœ€é©åŒ–ã®æŸ”è»Ÿæ€§ã‚’ç¢ºä¿
      R = Softmax(A Â· Aáµ€ / âˆšd)  where A âˆˆ {Q, K, V}
      L_AD = Î£_{Aâˆˆ{Q,K,V}} KL(R_teacher || R_student)

    åŠ¹æœ:
      - Logitsè’¸ç•™ã®ã¿: 87.32% â†’ +Attentionè’¸ç•™: 88.17% (MNLI)
      - å˜ä¸€ãƒ¬ã‚¤ãƒ¤ãƒ¼ > å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼è’¸ç•™
    """
    loss = 0
    for A_name in ['Q', 'K', 'V']:
        t_A = teacher_attn[layer_idx][A_name]  # (B, H, N, d)
        s_A = student_attn[layer_idx][A_name]

        # é–¢ä¿‚è¡Œåˆ—ã®è¨ˆç®—
        R_t = F.softmax(t_A @ t_A.transpose(-1, -2) / sqrt(d), dim=-1)
        R_s = F.softmax(s_A @ s_A.transpose(-1, -2) / sqrt(d), dim=-1)

        loss += F.kl_div(R_s.log(), R_t, reduction='batchmean')
    return loss
```

---

## å¾“æ¥ã®é‡å­åŒ–æ‰‹æ³•ã¨ã®æ¯”è¼ƒ

| æ‰‹æ³• | ãƒ“ãƒƒãƒˆæ•° | æ–¹å¼ | ä¹—ç®— | ç²¾åº¦ (MNLI) | ãƒ¡ãƒ¢ãƒªå‰Šæ¸› |
|------|----------|------|------|-------------|-----------|
| FP16 (ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³) | 16 | - | å¿…è¦ | 88.01% | 1x |
| INT8 (GPTQ) | 8 | Post-training | å¿…è¦ | ~87% | 2x |
| INT4 (AWQ) | 4 | Post-training | å¿…è¦ | ~85% | 4x |
| **BitNet b1.58** | **1.58** | **QAT** | **ä¸è¦** | **74.09%** (ç›´æ¥) | **~10x** |
| **BitDistill** | **1.58** | **è’¸ç•™+QAT** | **ä¸è¦** | **88.17%** | **~10x** |

### ãªãœä¹—ç®—ãŒä¸è¦ã«ãªã‚‹ã®ã‹

```
å¾“æ¥ (INT8):
  y = Î£ w_i Ã— x_i     â† ä¹—ç®—ãŒå¿…è¦

BitNet b1.58:
  w_i âˆˆ {-1, 0, 1} ãªã®ã§:
  y = Î£ w_i Ã— x_i
    = Î£_{w_i=1} x_i - Î£_{w_i=-1} x_i    â† åŠ æ¸›ç®—ã®ã¿!

å®Ÿéš›ã®ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã§ã®åŠ¹æœ:
  - ä¹—ç®—å™¨ä¸è¦ â†’ ãƒãƒƒãƒ—é¢ç©å‰Šæ¸›
  - åŠ æ¸›ç®—ã®ã¿ â†’ æ¶ˆè²»é›»åŠ›å¤§å¹…å‰Šæ¸›
  - ãƒ¡ãƒ¢ãƒªå¸¯åŸŸ â†’ 1.58 bits/weight ã§å¤§å¹…ç¯€ç´„
```

---

## å‡¦ç†ãƒ•ãƒ­ãƒ¼è©³ç´°

### æ¨è«–æ™‚ã®ãƒ•ãƒ­ãƒ¼

```python
def bitnet_inference(model, input_ids):
    """
    BitNet b1.58 ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ãƒ•ãƒ­ãƒ¼

    1. ãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿ (FP16 or Q6_K)
    2. å„Transformerãƒ¬ã‚¤ãƒ¤ãƒ¼:
       a. æ´»æ€§åŒ–ã‚’8-bitã«é‡å­åŒ–
       b. 1.58-bité‡ã¿ã¨è¡Œåˆ—ç© (åŠ æ¸›ç®—ã®ã¿)
       c. ã‚¹ã‚±ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã§å¾©å…ƒ
       d. SubLN â†’ æ®‹å·®æ¥ç¶š
    3. è¨€èªãƒ¢ãƒ‡ãƒ«ãƒ˜ãƒƒãƒ‰ã§æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬
    """
    # 1. Embedding
    x = model.embed_tokens(input_ids)  # (B, T, D)

    # 2. Transformer Layers
    for layer in model.layers:
        residual = x

        # Self-Attention
        x_norm = layer.input_layernorm(x)

        # æ´»æ€§åŒ–é‡å­åŒ–
        gamma = x_norm.abs().max(dim=-1, keepdim=True).values
        x_q = quantize_int8(x_norm, gamma)

        # Q, K, V è¨ˆç®— (1.58-bité‡ã¿ã¨ã®è¡Œåˆ—ç©)
        q = ternary_matmul(x_q, layer.q_proj.weight_ternary, layer.q_proj.scale)
        k = ternary_matmul(x_q, layer.k_proj.weight_ternary, layer.k_proj.scale)
        v = ternary_matmul(x_q, layer.v_proj.weight_ternary, layer.v_proj.scale)

        # Attentionè¨ˆç®—
        attn_output = scaled_dot_product_attention(q, k, v)

        # SubLN + å‡ºåŠ›æŠ•å½±
        attn_output = layer.sub_ln_attn(attn_output)
        attn_output = ternary_matmul(quantize_int8(attn_output),
                                      layer.o_proj.weight_ternary,
                                      layer.o_proj.scale)
        x = residual + attn_output

        # FFN
        residual = x
        x_norm = layer.post_attention_layernorm(x)
        x_q = quantize_int8(x_norm)

        gate = sigmoid(ternary_matmul(x_q, layer.gate_proj.weight_ternary, ...))
        up = ternary_matmul(x_q, layer.up_proj.weight_ternary, ...)
        ffn_out = layer.sub_ln_ffn(gate * up)
        x = residual + ternary_matmul(quantize_int8(ffn_out),
                                       layer.down_proj.weight_ternary, ...)

    # 3. Output
    logits = model.lm_head(model.norm(x))
    return logits
```

### C++ã§ã®æ¨è«–ã‚«ãƒ¼ãƒãƒ«ï¼ˆå…¬å¼å®Ÿè£…ã®æ¦‚è¦ï¼‰

```
å…¬å¼å®Ÿè£… (ggml-bitnet-mad.cpp) ã®å‡¦ç†ãƒ•ãƒ­ãƒ¼:

1. é‡ã¿ã®ãƒ‘ãƒƒã‚­ãƒ³ã‚°:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ 4ã¤ã®ä¸‰å€¤ â†’ 1ãƒã‚¤ãƒˆ (2 bits Ã— 4)    â”‚
   â”‚ -1â†’00, 0â†’01, 1â†’10                  â”‚
   â”‚ [w0|w1|w2|w3] = 1ãƒã‚¤ãƒˆ            â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

2. SIMDå±•é–‹ (AVX2):
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ 32ãƒã‚¤ãƒˆãƒ­ãƒ¼ãƒ‰ â†’ 128å€‹ã®ä¸‰å€¤å±•é–‹     â”‚
   â”‚ _mm256_and + _mm256_srli ã§2bitæŠ½å‡º â”‚
   â”‚ â†’ int8ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›                â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3. è¡Œåˆ—ãƒ™ã‚¯ãƒˆãƒ«ç©:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ _mm256_maddubs_epi16 ã§              â”‚
   â”‚ ä¸‰å€¤Ã—int8ã®ç©å’Œæ¼”ç®—                  â”‚
   â”‚ â†’ æ°´å¹³åŠ ç®—ã§æœ€çµ‚çµæœ                 â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

4. ä¸¦åˆ—åŒ–:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ ROW_BLOCK_SIZE Ã— COL_BLOCK_SIZE     â”‚
   â”‚ ã®ã‚¿ã‚¤ãƒ«ã§å‡¦ç†                       â”‚
   â”‚ PARALLEL_SIZEè¡Œã‚’åŒæ™‚å‡¦ç†            â”‚
   â”‚ â†’ ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡æœ€é©åŒ–              â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## BitDistillè’¸ç•™ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

### æ•°å¼ã¾ã¨ã‚

#### é‡ã¿é‡å­åŒ– (Eq. 1-2)
```
Q_w(W) = Î” Ã— RoundClip(W_FP16 / (Î” + Îµ), -1, 1)
Î” = mean(|W|)
RoundClip(Y, a, b) = min(max(round(Y), a), b)
```

#### æ´»æ€§åŒ–é‡å­åŒ– (Eq. 3)
```
Q_INT8(X) = (Î³/127) Ã— RoundClip(127Â·X_FP16 / (Î³+Îµ), -128, 127)
Î³ = max(|X_FP16|)
```

#### SubLNã‚’å«ã‚€Transformerãƒ¬ã‚¤ãƒ¤ãƒ¼ (Eq. 4-6)
```
Y_l = X_l + SubLN(Concat(heads)) Ã— W^MHSA_out
X_{l+1} = Y_l + SubLN((Y_l Ã— W^FFN_up) âŠ™ Ïƒ(Y_l Ã— W^FFN_gate)) Ã— W^FFN_down
heads = { Softmax(Q_i K_iáµ€ / âˆšd) Ã— V_i }
```

#### ç¶™ç¶šäº‹å‰å­¦ç¿’æå¤± (Eq. 7)
```
L_CT = -(1/N) Î£áµ¢ Î£â‚œ log P_Î¸(c_{i,t} | c_{i,<t})
```

#### Logitsè’¸ç•™æå¤± (Eq. 8-9)
```
L_LD = (1/N) Î£áµ¢ D_KL(P^FP16_Î¸(yáµ¢|xáµ¢) || P^1.58-bit_Î¸(yáµ¢|xáµ¢))
P_Î¸(y|x) = exp(z_y/Ï„) / Î£_{y'} exp(z_{y'}/Ï„)
```

#### Attentionè’¸ç•™æå¤± (Eq. 10-12)
```
L_AD = (1/|Î¥|) Î£áµ¢ Î£_{jâˆˆ{Q,K,V}} Î±áµ¢ Ã— (1/(A_rÂ·|x|)) Î£â‚ Î£â‚œ D_KL(R^FP16 || R^1.58-bit)
R = Softmax(A Â· Aáµ€ / âˆšd)   where A âˆˆ {Q, K, V}
```

#### ç·åˆæå¤± (Eq. 13-14)
```
L = L_CE + Î» Ã— L_LD + Î³ Ã— L_AD
L_CE = -(1/N) Î£áµ¢ Î£â‚œ log P_Î¸(yáµ¢áµ— | xáµ¢)
```

### ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | åˆ†é¡ã‚¿ã‚¹ã‚¯ | è¦ç´„ã‚¿ã‚¹ã‚¯ |
|-----------|-----------|-----------|
| Temperature (Ï„) | 5.0 | 5.0 |
| Î» (Logitsè’¸ç•™é‡ã¿) | 10 | 1 |
| Î³ (Attentionè’¸ç•™é‡ã¿) | 1e5 | 1e3 |
| Î±áµ¢ | 1.0 | 1.0 |
| Max sequence length | 512 | 512 |
| Batch size | 32 | 32 |
| ç¶™ç¶šäº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿é‡ | 10B tokens | 10B tokens |

### Ablation Study

| SubLN | ç¶™ç¶šäº‹å‰å­¦ç¿’ | è’¸ç•™ | MNLI Acc | CNNDM BLEU |
|-------|------------|------|----------|------------|
| âœ— | âœ— | âœ— | 74.09 | 11.47 |
| âœ“ | âœ— | âœ— | 76.30 | 11.69 |
| âœ“ | âœ“ | âœ— | 86.73 | 13.96 |
| âœ“ | âœ— | âœ“ | 88.04 | 13.70 |
| **âœ“** | **âœ“** | **âœ“** | **88.17** | **14.41** |

---

## å…¬å¼å®Ÿè£…ã®æ§‹æˆ

### ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

```
BitNet/
â”œâ”€â”€ src/                              # C++ã‚³ã‚¢å®Ÿè£…
â”‚   â”œâ”€â”€ ggml-bitnet-mad.cpp          # é‡å­åŒ– + SIMDæ¨è«–ã‚«ãƒ¼ãƒãƒ« (1055è¡Œ)
â”‚   â”œâ”€â”€ ggml-bitnet-lut.cpp          # LUTæ–¹å¼ã‚«ãƒ¼ãƒãƒ«ãƒ©ãƒƒãƒ‘ãƒ¼ (166è¡Œ)
â”‚   â””â”€â”€ README.md                     # CPUæœ€é©åŒ–ã‚¬ã‚¤ãƒ‰
â”œâ”€â”€ include/                          # C++ãƒ˜ãƒƒãƒ€ãƒ¼
â”‚   â”œâ”€â”€ ggml-bitnet.h                # ãƒ‘ãƒ–ãƒªãƒƒã‚¯API
â”‚   â””â”€â”€ gemm-config.h                # ã‚¿ã‚¤ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
â”œâ”€â”€ gpu/                             # GPU/CUDAå®Ÿè£…
â”‚   â”œâ”€â”€ model.py                     # BitNetãƒ¢ãƒ‡ãƒ«å®šç¾© (PyTorch)
â”‚   â”œâ”€â”€ generate.py                  # GPUæ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”‚   â”œâ”€â”€ bitnet_kernels/
â”‚   â”‚   â”œâ”€â”€ bitnet_kernels.cu        # CUDAã‚«ãƒ¼ãƒãƒ« (W2A8 GEMV)
â”‚   â”‚   â””â”€â”€ setup.py                 # ã‚«ãƒ¼ãƒãƒ«ãƒ“ãƒ«ãƒ‰
â”‚   â”œâ”€â”€ pack_weight.py               # é‡ã¿ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
â”‚   â””â”€â”€ convert_checkpoint.py        # ãƒ¢ãƒ‡ãƒ«å¤‰æ›
â”œâ”€â”€ utils/                           # å¤‰æ›ãƒ»ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”‚   â”œâ”€â”€ convert-hf-to-gguf-bitnet.py # HuggingFace â†’ GGUFå¤‰æ›
â”‚   â”œâ”€â”€ codegen_tl1.py              # ARM TL1ã‚«ãƒ¼ãƒãƒ«ç”Ÿæˆ
â”‚   â”œâ”€â”€ codegen_tl2.py              # x86 TL2ã‚«ãƒ¼ãƒãƒ«ç”Ÿæˆ
â”‚   â”œâ”€â”€ quantize_embeddings.py       # Embeddingé‡å­åŒ– (Q6_K)
â”‚   â””â”€â”€ e2e_benchmark.py             # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
â”œâ”€â”€ 3rdparty/llama.cpp/              # llama.cppæ¨è«–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
â”œâ”€â”€ preset_kernels/                  # äº‹å‰ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ã‚«ãƒ¼ãƒãƒ«
â”œâ”€â”€ run_inference.py                 # CPUæ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ setup_env.py                     # ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
â””â”€â”€ CMakeLists.txt                   # ãƒ“ãƒ«ãƒ‰è¨­å®š
```

### è¨€èªæ§‹æˆ

| è¨€èª | ç”¨é€” | ä¸»è¦ãƒ•ã‚¡ã‚¤ãƒ« |
|------|------|------------|
| **C++** | ã‚³ã‚¢æ¨è«–ã‚«ãƒ¼ãƒãƒ« (SIMDæœ€é©åŒ–) | `src/ggml-bitnet-mad.cpp` |
| **CUDA** | GPUæ¨è«–ã‚«ãƒ¼ãƒãƒ« (dp4a) | `gpu/bitnet_kernels/bitnet_kernels.cu` |
| **Python** | ãƒ¢ãƒ‡ãƒ«å¤‰æ›ã€GPUæ¨è«–ãƒ©ãƒƒãƒ‘ãƒ¼ | `gpu/model.py`, `utils/*.py` |
| **CMake** | ãƒ“ãƒ«ãƒ‰ã‚·ã‚¹ãƒ†ãƒ  | `CMakeLists.txt` |

### å¯¾å¿œãƒ¢ãƒ‡ãƒ«

| ãƒ¢ãƒ‡ãƒ« | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° | x86 I2_S | x86 TL2 | ARM TL1 | ARM I2_S |
|--------|-------------|----------|---------|---------|----------|
| BitNet-b1.58-2B-4T | 2.4B | âœ“ | âœ“ | âœ“ | âœ“ |
| bitnet_b1_58-large | 0.7B | âœ“ | âœ“ | âœ“ | âœ“ |
| bitnet_b1_58-3B | 3.3B | âœ— | âœ“ | âœ“ | âœ— |
| Llama3-8B-1.58 | 8.0B | âœ“ | âœ“ | âœ“ | âœ“ |
| Falcon3 Family | 1B-10B | âœ“ | âœ“ | âœ“ | âœ“ |

---

## å½¢çŠ¶ã‚¬ã‚¤ãƒ‰

### é‡å­åŒ–é–¢é€£

| æ®µéš | åç§° | å½¢çŠ¶ | èª¬æ˜ |
|------|------|------|------|
| **é‡ã¿** | W_fp16 | `(out_features, in_features)` | å…ƒã®FP16é‡ã¿ |
| | W_ternary | `(out_features, in_features)` | ä¸‰å€¤ {-1,0,1} |
| | W_packed | `(out_features, in_features/4)` | ãƒ‘ãƒƒã‚¯æ¸ˆã¿ (2bitÃ—4/byte) |
| | scale | `(1,)` or `(out_features,)` | per-tensor or per-channel |
| **æ´»æ€§åŒ–** | X_fp16 | `(B, T, D)` | å…ƒã®FP16æ´»æ€§åŒ– |
| | X_int8 | `(B, T, D)` | 8-bité‡å­åŒ–æ¸ˆã¿ |
| | gamma | `(B, T, 1)` | per-token absmax |

### Transformerãƒ¬ã‚¤ãƒ¤ãƒ¼

| æ®µéš | åç§° | å½¢çŠ¶ | èª¬æ˜ |
|------|------|------|------|
| **å…¥åŠ›** | input_ids | `(B, T)` | ãƒˆãƒ¼ã‚¯ãƒ³ID |
| **Embedding** | x | `(B, T, D)` | åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ« |
| **Attention** | Q, K, V | `(B, H, T, d_head)` | Query/Key/Value |
| | attn_weights | `(B, H, T, T)` | Attentioné‡ã¿ |
| | attn_output | `(B, T, D)` | Attentionå‡ºåŠ› |
| **FFN** | gate | `(B, T, D_ffn)` | GateæŠ•å½± |
| | up | `(B, T, D_ffn)` | UpæŠ•å½± |
| | down_output | `(B, T, D)` | DownæŠ•å½±å‡ºåŠ› |
| **å‡ºåŠ›** | logits | `(B, T, V)` | èªå½™ä¸Šã®ç¢ºç‡åˆ†å¸ƒ |

### è’¸ç•™é–¢é€£

| æ®µéš | åç§° | å½¢çŠ¶ | èª¬æ˜ |
|------|------|------|------|
| **Logitsè’¸ç•™** | t_logits | `(B, T, V)` | Teacher logits |
| | s_logits | `(B, T, V)` | Student logits |
| | t_probs | `(B, T, V)` | Softmax(t_logits/Ï„) |
| **Attnè’¸ç•™** | A_teacher | `(B, H, T, d)` | Teacher Q/K/V |
| | A_student | `(B, H, T, d)` | Student Q/K/V |
| | R_teacher | `(B, H, T, T)` | é–¢ä¿‚è¡Œåˆ— (teacher) |
| | R_student | `(B, H, T, T)` | é–¢ä¿‚è¡Œåˆ— (student) |

è»¸ã®æ„å‘³:
- **B**: ãƒãƒƒãƒã‚µã‚¤ã‚º
- **T**: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
- **D**: éš ã‚Œå±¤æ¬¡å…ƒ (ä¾‹: Qwen3-0.6B = 1024)
- **H**: Attention Headæ•°
- **d_head**: Headæ¬¡å…ƒ = D / H
- **D_ffn**: FFNä¸­é–“æ¬¡å…ƒ (é€šå¸¸ D Ã— 8/3)
- **V**: èªå½™ã‚µã‚¤ã‚º

---

## Qwen3-0.6Bã¸ã®é©ç”¨ã‚µãƒ³ãƒ—ãƒ«

[qwen3_finetune_example.py](qwen3_finetune_example.py) ã«å®Œå…¨ãªå®Ÿè£…ä¾‹ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚

æ¦‚è¦:
1. Qwen3-0.6B (FP16) ã‚’Teacherã¨ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
2. BitNetå¤‰æ›: å…¨Linearå±¤ã‚’ `BitLinear` ã«ç½®ãæ›ãˆ + SubLNæŒ¿å…¥
3. ç¶™ç¶šäº‹å‰å­¦ç¿’: å°è¦æ¨¡ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§é‡ã¿åˆ†å¸ƒã‚’å¤‰æ›
4. è’¸ç•™ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°: Teacherâ†’Student ã®çŸ¥è­˜è’¸ç•™

---

## FAQ

### Q1: BitNet b1.58 ã® "1.58" ã¯ä½•ã‚’æ„å‘³ã™ã‚‹ã®ã‹ï¼Ÿ

**A**: ä¸‰å€¤ {-1, 0, 1} ã‚’è¡¨ç¾ã™ã‚‹ã®ã«å¿…è¦ãªæƒ…å ±é‡ã¯ logâ‚‚(3) â‰ˆ 1.585 bits ã§ã™ã€‚å®Ÿéš›ã«ã¯per-tensorã®ã‚¹ã‚±ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ã‚‚ä¿å­˜ã™ã‚‹ãŸã‚ã€å¹³å‡ã™ã‚‹ã¨ç´„1.58 bits/weight ã¨ãªã‚Šã¾ã™ã€‚

**è£œè¶³**: å®Ÿè£…ä¸Šã¯2 bits/value ã§ãƒ‘ãƒƒã‚­ãƒ³ã‚°ï¼ˆ4å€¤ã‚’1ãƒã‚¤ãƒˆï¼‰ã—ã¾ã™ãŒã€ç†è«–çš„ãªæƒ…å ±é‡ã¯1.58 bitsã§ã™ã€‚

---

### Q2: ãªãœä¹—ç®—ãŒä¸è¦ã«ãªã‚‹ã®ã‹ï¼Ÿ

**A**: é‡ã¿ãŒ {-1, 0, 1} ã®ã¿ãªã®ã§ã€`w Ã— x` ã¯ä»¥ä¸‹ã«å¤‰æ›å¯èƒ½ã§ã™ï¼š
- w = 1: `+x`ï¼ˆåŠ ç®—ï¼‰
- w = -1: `-x`ï¼ˆæ¸›ç®—ï¼‰
- w = 0: ä½•ã‚‚ã—ãªã„ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰

**åŠ¹æœ**: ä¹—ç®—å™¨ã¯åŠ ç®—å™¨ã‚ˆã‚Šé¢ç©ãƒ»æ¶ˆè²»é›»åŠ›ãŒå¤§ãã„ãŸã‚ã€ãƒãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã§å¤§å¹…ãªçœé›»åŠ›åŒ–ãŒå¯èƒ½ã§ã™ã€‚å®Ÿéš›ã«ã¯SIMDã® `maddubs` å‘½ä»¤ã§ int8Ã—int8 ã®ç©å’Œã‚’è¡Œã„ã¾ã™ãŒã€ä¸€æ–¹ãŒ {-1,0,1} ã®ãŸã‚å®Ÿè³ªçš„ã«åŠ æ¸›ç®—ã§ã™ã€‚

---

### Q3: BitDistillã¨ç›´æ¥QATã®é•ã„ã¯ï¼Ÿ

**A**: ç›´æ¥QATï¼ˆQuantization-Aware Trainingï¼‰ã¯FP16ãƒ¢ãƒ‡ãƒ«ã‚’ãã®ã¾ã¾1.58-bitã§å†å­¦ç¿’ã—ã¾ã™ãŒã€ç²¾åº¦ãŒå¤§å¹…ã«åŠ£åŒ–ã—ã¾ã™ï¼ˆMNLI: 74.09%ï¼‰ã€‚BitDistillã¯3æ®µéšã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ç²¾åº¦åŠ£åŒ–ã‚’æœ€å°åŒ–ã—ã¾ã™ï¼ˆMNLI: 88.17%ï¼‰ã€‚

**éµ**: ç¶™ç¶šäº‹å‰å­¦ç¿’ã§é‡ã¿åˆ†å¸ƒã‚’BitNetå‘ãã«å¤‰æ›ã—ã€è’¸ç•™ã§FP16ã®çŸ¥è­˜ã‚’å¼•ãç¶™ãã“ã¨ã§ã€ç›´æ¥QATã®14ãƒã‚¤ãƒ³ãƒˆã®ç²¾åº¦ã‚®ãƒ£ãƒƒãƒ—ã‚’è§£æ¶ˆã—ã¦ã„ã¾ã™ã€‚

---

### Q4: ç¶™ç¶šäº‹å‰å­¦ç¿’ã¯ãªãœå¿…è¦ãªã®ã‹ï¼Ÿ

**A**: FP16ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿åˆ†å¸ƒã¯ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã«è¿‘ã„å½¢çŠ¶ã§ã™ã€‚ä¸‰å€¤é‡å­åŒ–ã®é·ç§»å¢ƒç•Œï¼ˆ0ã¨Â±1ã®é–“ï¼‰ã‹ã‚‰é›¢ã‚ŒãŸé‡ã¿ãŒå¤šã„ãŸã‚ã€å°ã•ãªå‹¾é…æ›´æ–°ã§ã¯é‡å­åŒ–å€¤ãŒå¤‰åŒ–ã—ã¾ã›ã‚“ã€‚

ç¶™ç¶šäº‹å‰å­¦ç¿’ï¼ˆ10B tokensï¼‰ã§é‡ã¿åˆ†å¸ƒã‚’å¤‰æ›ã—ã€é·ç§»å¢ƒç•Œä»˜è¿‘ã«é‡ã¿ã‚’é›†ä¸­ã•ã›ã‚‹ã“ã¨ã§ã€å‹¾é…ãƒ™ãƒ¼ã‚¹ã®æœ€é©åŒ–ãŒåŠ¹æœçš„ã«æ©Ÿèƒ½ã™ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

**åŠ¹æœ**: SubLNã®ã¿: 76.30% â†’ +ç¶™ç¶šäº‹å‰å­¦ç¿’: 86.73% (MNLI, +10.4ãƒã‚¤ãƒ³ãƒˆ)

---

### Q5: å…¬å¼å®Ÿè£…ã¯C++ã‹Pythonã‹ï¼Ÿ

**A**: ä¸¡æ–¹ã§ã™ã€‚

- **CPUæ¨è«–**: C++ (SIMDæœ€é©åŒ–ã€llama.cpp ãƒ™ãƒ¼ã‚¹)
  - `src/ggml-bitnet-mad.cpp`: AVX2/NEON ã«ã‚ˆã‚‹é«˜é€Ÿã‚«ãƒ¼ãƒãƒ«
  - `src/ggml-bitnet-lut.cpp`: ãƒ«ãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ†ãƒ¼ãƒ–ãƒ«æ–¹å¼
- **GPUæ¨è«–**: CUDA (C++) + Python (PyTorch ãƒ©ãƒƒãƒ‘ãƒ¼)
  - `gpu/bitnet_kernels/bitnet_kernels.cu`: W2A8 GEMV ã‚«ãƒ¼ãƒãƒ«
  - `gpu/model.py`: PyTorch ãƒ¢ãƒ‡ãƒ«å®šç¾©
- **ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£**: Python
  - ãƒ¢ãƒ‡ãƒ«å¤‰æ›ã€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ

æ¨è«–ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãªéƒ¨åˆ†ã¯C++/CUDAã§å®Ÿè£…ã•ã‚Œã€ãƒ¢ãƒ‡ãƒ«å¤‰æ›ã‚„ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¯Pythonã§è¡Œã„ã¾ã™ã€‚

---

### Q6: ã©ã®ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã§BitDistillã¯æœ‰åŠ¹ã‹ï¼Ÿ

**A**: è«–æ–‡ã§ã¯Qwen3ã®0.6Bã€1.7Bã€4Bã§æ¤œè¨¼ã•ã‚Œã¦ã„ã¾ã™ã€‚å…¨ã‚µã‚¤ã‚ºã§FP16ã¨ã»ã¼åŒç­‰ã®ç²¾åº¦ã‚’é”æˆã—ã¦ã„ã¾ã™ï¼š

| ãƒ¢ãƒ‡ãƒ« | FP16 (MNLI) | BitDistill (MNLI) | å·®åˆ† |
|--------|-------------|-------------------|------|
| Qwen3-0.6B | 88.01 | 88.17 | +0.16 |
| Qwen3-1.7B | 89.61 | 89.53 | -0.08 |
| Qwen3-4B | 91.48 | 91.40 | -0.08 |

ã¾ãŸã€Qwen2.5-0.5Bã‚„Gemma3-1Bã§ã‚‚æœ‰åŠ¹æ€§ãŒç¢ºèªã•ã‚Œã¦ã„ã¾ã™ã€‚

---

### Q7: ã‚ˆã‚Šå¤§ããªTeacherãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã†ã¨ã©ã†ãªã‚‹ã‹ï¼Ÿ

**A**: å¤§ããªTeacherã»ã©1.58-bit Studentã®æ€§èƒ½ãŒå‘ä¸Šã—ã¾ã™ã€‚Qwen3-0.6B (1.58-bit) ã«å¯¾ã—ã¦ï¼š

- Qwen3-0.6B Teacher â†’ 88.17% (MNLI)
- Qwen3-4B Teacher â†’ ã‚ˆã‚Šé«˜ã„ç²¾åº¦

ã¤ã¾ã‚Šã€1.58-bit 0.6Bãƒ¢ãƒ‡ãƒ«ãŒFP16 0.6Bã‚’è¶…ãˆã‚‹æ€§èƒ½ã‚’é”æˆå¯èƒ½ã§ã™ã€‚

---

### Q8: Embeddingå±¤ã‚‚é‡å­åŒ–ã•ã‚Œã‚‹ã®ã‹ï¼Ÿ

**A**: å…¬å¼æ¨è«–å®Ÿè£…ã§ã¯ã€Embeddingå±¤ã¯Q6_Kå½¢å¼ã§é‡å­åŒ–å¯èƒ½ã§ã™ï¼ˆ6-bité‡å­åŒ–ï¼‰ã€‚

- F32: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
- Q6_K: ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£å¢—åŠ  <0.5%ã€ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ1.2å€å‘ä¸Š
- Q4_0: ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£å¢—åŠ  ~3-5%ï¼ˆæ¨å¥¨ã—ãªã„ï¼‰

BitDistillè«–æ–‡ã§ã¯Embeddingé‡å­åŒ–ã¯å¯¾è±¡å¤–ã§ã€é‡ã¿è¡Œåˆ—ï¼ˆLinearå±¤ï¼‰ã®ã¿1.58-bitåŒ–ã—ã¾ã™ã€‚

---

### Q9: STE (Straight-Through Estimator) ã¨ã¯ä½•ã‹ï¼Ÿ

**A**: ä¸‰å€¤é‡å­åŒ–ã®RoundClipé–¢æ•°ã¯ä¸é€£ç¶šï¼ˆå‹¾é…ãŒã»ã¼0ï¼‰ãªã®ã§ã€é€šå¸¸ã®é€†ä¼æ’­ã§ã¯å­¦ç¿’ã§ãã¾ã›ã‚“ã€‚STEã¯é †ä¼æ’­ã§ã¯é‡å­åŒ–ã‚’é©ç”¨ã—ã€é€†ä¼æ’­ã§ã¯é‡å­åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦å‹¾é…ã‚’ãã®ã¾ã¾é€šã™æ‰‹æ³•ã§ã™ã€‚

```python
# é †ä¼æ’­: w_q = RoundClip(w / Î”, -1, 1)
# é€†ä¼æ’­: âˆ‚L/âˆ‚w â‰ˆ âˆ‚L/âˆ‚w_q  (é‡å­åŒ–ã‚’ç„¡è¦–)
w_q = w + (RoundClip(w / delta, -1, 1) * delta - w).detach()
```

ã“ã‚Œã«ã‚ˆã‚Šã€é›¢æ•£çš„ãªé‡å­åŒ–é–¢æ•°ã‚’é€šã—ã¦å‹¾é…ãƒ™ãƒ¼ã‚¹ã®æœ€é©åŒ–ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

---

### Q10: å®Ÿéš›ã®ãƒ‡ãƒ—ãƒ­ã‚¤æ™‚ã®æ¨è«–é€Ÿåº¦ã¯ã©ã®ç¨‹åº¦ã‹ï¼Ÿ

**A**: å…¬å¼ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼š

**CPUæ¨è«– (BitNet-b1.58-2B, AMD EPYC 7V13):**
- 1ã‚¹ãƒ¬ãƒƒãƒ‰: ~1.4 tokens/sec
- 8ã‚¹ãƒ¬ãƒƒãƒ‰: ~8 tokens/sec
- 16ã‚¹ãƒ¬ãƒƒãƒ‰: 1,135 tokens/sec (FP16ã®2.65å€)

**GPUæ¨è«– (A100):**
- W2A8ã‚«ãƒ¼ãƒãƒ«: 13-30Î¼s (BF16æ¯” 2.89-3.27å€é«˜é€Ÿ)
- ãƒ¡ãƒ¢ãƒª: FP16ã®ç´„1/10

100Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã§ã‚‚å˜ä¸€CPUã§5-7 tokens/secã®æ¨è«–ãŒå¯èƒ½ã§ã™ã€‚

---

## ã¾ã¨ã‚

**BitNet b1.58 ã®ãƒã‚¤ãƒ³ãƒˆ:**
1. é‡ã¿ã‚’ {-1, 0, 1} ã«é‡å­åŒ–ã—ã€ä¹—ç®—ã‚’åŠ æ¸›ç®—ã«å¤‰æ›
2. ç†è«–çš„ã«1.58 bits/weight ã§FP16ã®ç´„1/10ã®ãƒ¡ãƒ¢ãƒª
3. CPUä¸Šã§2.65å€ã€GPUä¸Šã§3.27å€ã®é«˜é€ŸåŒ–

**BitDistill ã®ãƒã‚¤ãƒ³ãƒˆ:**
1. æ—¢å­˜FP16ãƒ¢ãƒ‡ãƒ«ã‚’1.58-bitã«å¤‰æ›ã™ã‚‹3æ®µéšãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
2. SubLN â†’ ç¶™ç¶šäº‹å‰å­¦ç¿’ â†’ è’¸ç•™ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
3. FP16ã¨ã»ã¼åŒç­‰ã®ç²¾åº¦ã‚’1.58-bitã§é”æˆ
4. ã‚³ã‚¹ãƒˆ: 10B tokensï¼ˆãƒ•ãƒ«ã‚¹ã‚¯ãƒ©ãƒƒãƒã®0.25%ï¼‰ã§ç¶™ç¶šäº‹å‰å­¦ç¿’

**å…¬å¼å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆ:**
1. C++ (SIMD) + CUDA + Python ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ§‹æˆ
2. llama.cpp ãƒ™ãƒ¼ã‚¹ã§æ—¢å­˜ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã¨çµ±åˆ
3. AVX2/NEON/dp4a ã«ã‚ˆã‚‹é«˜åº¦ãªæœ€é©åŒ–
4. è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ãƒ»ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«å¯¾å¿œ

---

## å‚è€ƒæ–‡çŒ®

1. Wu, X. et al. "BitNet Distillation." arXiv:2510.13998, 2025.
2. Ma, S. et al. "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits." arXiv:2402.17764, 2024.
3. Wang, H. et al. "BitNet: Scaling 1-bit Transformers for Large Language Models." arXiv:2310.11453, 2023.
4. Wang, W. et al. "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers." NeurIPS 2020.
5. Microsoft BitNet å…¬å¼å®Ÿè£…: https://github.com/microsoft/BitNet
