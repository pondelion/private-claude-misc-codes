# LightGlue Understanding - ç°¡ç•¥åŒ–ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰é›†

LightGlue (Local Feature Matching at Light Speed) ã®ç†è§£ã‚’ç›®çš„ã¨ã—ãŸç°¡ç•¥åŒ–ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰é›†ã§ã™ã€‚

è«–æ–‡: [LightGlue: Local Feature Matching at Light Speed](https://arxiv.org/abs/2306.13643) (ICCV 2023)

## ğŸ“‹ ç›®æ¬¡

- [æ¦‚è¦](#æ¦‚è¦)
- [ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å…¨ä½“åƒ](#ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å…¨ä½“åƒ)
- [ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ](#ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ)
- [LightGlueã®ä¸»è¦ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³](#lightglueã®ä¸»è¦ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³)
- [SuperGlueã¨ã®æ¯”è¼ƒ](#superglueã¨ã®æ¯”è¼ƒ)
- [å‡¦ç†ãƒ•ãƒ­ãƒ¼è©³ç´°](#å‡¦ç†ãƒ•ãƒ­ãƒ¼è©³ç´°)
- [å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ](#å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ)
- [å½¢çŠ¶ã‚¬ã‚¤ãƒ‰](#å½¢çŠ¶ã‚¬ã‚¤ãƒ‰)
- [FAQ](#faq)

---

## æ¦‚è¦

**LightGlueã®ç‰¹å¾´:**
- **é«˜é€Ÿ**: SuperGlueã®2.5å€ä»¥ä¸Šé«˜é€Ÿï¼ˆAdaptiveæ©Ÿæ§‹ã«ã‚ˆã‚Šï¼‰
- **é«˜ç²¾åº¦**: SuperGlueã¨åŒç­‰ä»¥ä¸Šã®ç²¾åº¦
- **å­¦ç¿’å®¹æ˜“**: 2 GPU-daysã§è¨“ç·´å¯èƒ½ï¼ˆSuperGlueã¯7+ daysï¼‰
- **é©å¿œçš„**: ç”»åƒãƒšã‚¢ã®é›£æ˜“åº¦ã«å¿œã˜ã¦è¨ˆç®—é‡ã‚’èª¿æ•´

**ã‚¿ã‚¹ã‚¯:**
- Local Feature Matchingï¼ˆå±€æ‰€ç‰¹å¾´é‡ãƒãƒƒãƒãƒ³ã‚°ï¼‰
- Sparse Correspondence Estimationï¼ˆã‚¹ãƒ‘ãƒ¼ã‚¹å¯¾å¿œæ¨å®šï¼‰
- Outlier Rejectionï¼ˆå¤–ã‚Œå€¤é™¤å»ï¼‰

**æ€§èƒ½** (MegaDepth-1500, SuperPoint features):
| Method | AUC@5Â° | AUC@10Â° | AUC@20Â° | Time (ms) |
|--------|--------|---------|---------|-----------|
| SuperGlue | 49.7 | 67.1 | 80.6 | 70.0 |
| SGMNet | 43.2 | 61.6 | 75.6 | 73.8 |
| **LightGlue** | **49.9** | **67.0** | **80.1** | **44.2** |
| LightGlue (adaptive) | 49.4 | 67.2 | 80.1 | **31.4** |

---

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å…¨ä½“åƒ

```
å…¥åŠ›: 2ã¤ã®ç”»åƒã‹ã‚‰ã®å±€æ‰€ç‰¹å¾´é‡
    Image A: keypoints (M, 2), descriptors (M, D)
    Image B: keypoints (N, 2), descriptors (N, D)
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Input Projection                          â”‚
â”‚    descriptors â†’ state vectors (256-dim)     â”‚
â”‚    Linear(D, 256) if D != 256                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Positional Encoding (Rotary)              â”‚
â”‚    Learnable Fourier Features                â”‚
â”‚    positions â†’ rotary embeddings             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Transformer Layers Ã— L (L=9)                         â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚    â”‚ Self-Attention (Image A)            â”‚â† Rotary PE   â”‚
â”‚    â”‚ Self-Attention (Image B)            â”‚â† Rotary PE   â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                     â†“                                   â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚    â”‚ Cross-Attention (Aâ†”B)               â”‚ Bidirectionalâ”‚
â”‚    â”‚ é¡ä¼¼åº¦è¡Œåˆ—ã‚’ä¸€åº¦ã ã‘è¨ˆç®—              â”‚              â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                     â†“                                   â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚    â”‚ Confidence Classifier               â”‚              â”‚
â”‚    â”‚ â†’ æ—©æœŸçµ‚äº†åˆ¤å®š (Adaptive Depth)     â”‚              â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                     â†“                                   â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚    â”‚ Point Pruning                       â”‚              â”‚
â”‚    â”‚ â†’ ãƒãƒƒãƒä¸å¯èƒ½ç‚¹ã‚’é™¤å¤– (Adaptive Width)â”‚              â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Matching Head                             â”‚
â”‚    - Similarity Matrix: S = proj(x_A)áµ€ proj(x_B)  â”‚
â”‚    - Matchability: Ïƒ = sigmoid(Linear(x))    â”‚
â”‚    - Assignment: P = Ïƒ_A Ïƒ_B softmax(S) softmax(Sáµ€) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Match Filtering                           â”‚
â”‚    - Mutual nearest neighbor check          â”‚
â”‚    - Threshold filtering (Ï„=0.1)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
å‡ºåŠ›:
    matches: List of (i, j) correspondences
    scores: Matching confidence scores
    stop: Layer at which inference stopped
```

---

## ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

### 1. [main_flow.py](main_flow.py)
**LightGlueã®å…¨ä½“ãƒ•ãƒ­ãƒ¼**

ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹ `LightGlue` ã§ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†ã‚’å®Ÿè£…:
- Input Projectionï¼ˆæ¬¡å…ƒå¤‰æ›ï¼‰
- Positional Encodingï¼ˆRotary PEï¼‰
- Transformer Layersï¼ˆSelf + Cross Attentionï¼‰
- Adaptive Inferenceï¼ˆEarly Stop + Pruningï¼‰
- Matching Headï¼ˆAssignmentäºˆæ¸¬ï¼‰

```python
class LightGlue(nn.Module):
    def forward(self, data):
        # data = {'image0': {...}, 'image1': {...}}

        # 1. ç‰¹å¾´é‡æŠ½å‡º
        kpts0, kpts1 = data['image0']['keypoints'], data['image1']['keypoints']
        desc0, desc1 = data['image0']['descriptors'], data['image1']['descriptors']

        # 2. ä½ç½®æ­£è¦åŒ– & ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        kpts0 = normalize_keypoints(kpts0, size0)  # [-1, 1]
        encoding0 = self.posenc(kpts0)  # Rotary embeddings

        # 3. Transformer Layers
        for i in range(self.n_layers):
            desc0, desc1 = self.transformers[i](desc0, desc1, encoding0, encoding1)

            # Early stopping check
            if self.check_if_stop(token0, token1, i):
                break

            # Point pruning
            if should_prune:
                desc0 = desc0[keep_mask0]
                desc1 = desc1[keep_mask1]

        # 4. Matching
        scores = self.log_assignment[i](desc0, desc1)
        matches = filter_matches(scores, threshold=0.1)

        return {'matches': matches, 'scores': scores, 'stop': i+1}
```

---

### 2. [transformer_blocks.py](transformer_blocks.py)
**Transformeræ§‹æˆè¦ç´ **

#### ğŸ”‘ **ã‚­ãƒ¼ãƒ»ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³: Rotary Positional Encoding**

```python
class LearnableFourierPositionalEncoding(nn.Module):
    """
    å¾“æ¥æ‰‹æ³• (SuperGlue):
      - çµ¶å¯¾ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: MLP(p) â†’ x ã«åŠ ç®—
      - æ·±ã„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§ä½ç½®æƒ…å ±ãŒè–„ã‚Œã‚‹

    LightGlue:
      - ç›¸å¯¾ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° (Rotary)
      - å„self-attentionãƒ¬ã‚¤ãƒ¤ãƒ¼ã§query/keyã«é©ç”¨
      - ä½ç½®æƒ…å ±ãŒå¸¸ã«ä¿æŒã•ã‚Œã‚‹

    æ•°å­¦çš„è¡¨ç¾:
      R(p) = diag(RÌ‚(bâ‚áµ€p), RÌ‚(bâ‚‚áµ€p), ...)
      RÌ‚(Î¸) = [[cos Î¸, -sin Î¸], [sin Î¸, cos Î¸]]

      attention_score = qáµ¢áµ€ R(pâ±¼ - páµ¢) kâ±¼
    """

    def forward(self, positions):
        # positions: (B, N, 2) normalized to [-1, 1]
        projected = self.Wr(positions)  # (B, N, F_dim//2)
        cosines = torch.cos(projected)
        sines = torch.sin(projected)
        return torch.stack([cosines, sines], 0)  # (2, B, N, F_dim)
```

#### ğŸ”‘ **ã‚­ãƒ¼ãƒ»ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³: Bidirectional Cross-Attention**

```python
class CrossBlock(nn.Module):
    """
    å¾“æ¥æ‰‹æ³•:
      - Aâ†’Bã®é¡ä¼¼åº¦: sim_AB = q_A @ k_Báµ€
      - Bâ†’Aã®é¡ä¼¼åº¦: sim_BA = q_B @ k_Aáµ€
      - è¨ˆç®—é‡: O(2 Ã— N Ã— M Ã— d)

    LightGlue:
      - å…±æœ‰key: k_A = k_B ã®æŠ•å½±ã‚’å…±æœ‰
      - sim_AB = k_Aáµ€ @ k_B = sim_BAáµ€
      - è¨ˆç®—é‡: O(N Ã— M Ã— d) â† åŠåˆ†!

    åŠ¹æœ:
      - 20%ã®é«˜é€ŸåŒ–
      - ç²¾åº¦ã«å½±éŸ¿ãªã—
    """

    def forward(self, x0, x1):
        # å…±æœ‰key projection
        qk0 = self.to_qk(x0)  # Query/Key shared
        qk1 = self.to_qk(x1)
        v0 = self.to_v(x0)
        v1 = self.to_v(x1)

        # é¡ä¼¼åº¦è¡Œåˆ—ã‚’ä¸€åº¦ã ã‘è¨ˆç®—
        sim = einsum('bhid, bhjd -> bhij', qk0, qk1)

        # åŒæ–¹å‘ã®attention
        attn01 = softmax(sim, dim=-1)      # Aâ†’B
        attn10 = softmax(sim.T, dim=-1).T  # Bâ†’A (è»¢ç½®ã§è¨ˆç®—)

        m0 = einsum('bhij, bhjd -> bhid', attn01, v1)
        m1 = einsum('bhji, bhjd -> bhid', attn10.T, v0)

        return x0 + FFN([x0, m0]), x1 + FFN([x1, m1])
```

---

### 3. [matching_head.py](matching_head.py)
**ãƒãƒƒãƒãƒ³ã‚°äºˆæ¸¬ãƒ˜ãƒƒãƒ‰**

#### ğŸ”‘ **ã‚­ãƒ¼ãƒ»ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³: Double Softmax + Matchability**

```python
class MatchAssignment(nn.Module):
    """
    å¾“æ¥æ‰‹æ³• (SuperGlue):
      - Sinkhorn Algorithm (æœ€é©è¼¸é€å•é¡Œ)
      - 100ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¿…è¦
      - Dustbin ã§ unmatchable ã‚’è¡¨ç¾
      - è¨ˆç®—é‡å¤§ã€ãƒ¡ãƒ¢ãƒªå¤§

    LightGlue:
      - Double Softmax (è¡Œãƒ»åˆ—ä¸¡æ–¹å‘)
      - Matchability score (unary)
      - 1å›ã®è¨ˆç®—ã§å®Œäº†
      - è¨ˆç®—é‡å°ã€ãƒ¡ãƒ¢ãƒªå°ã€å‹¾é…ã‚¯ãƒªãƒ¼ãƒ³

    æ•°å­¦çš„è¡¨ç¾:
      Similarity: Sáµ¢â±¼ = Linear(xá´¬áµ¢)áµ€ Linear(xá´®â±¼)
      Matchability: Ïƒáµ¢ = sigmoid(Linear(xáµ¢))
      Assignment: Páµ¢â±¼ = Ïƒá´¬áµ¢ Ã— Ïƒá´®â±¼ Ã— softmax(S)áµ¢ Ã— softmax(Sáµ€)â±¼
    """

    def forward(self, desc0, desc1):
        # Similarity matrix
        mdesc0 = self.final_proj(desc0) / d**0.25
        mdesc1 = self.final_proj(desc1) / d**0.25
        sim = einsum('bmd, bnd -> bmn', mdesc0, mdesc1)

        # Matchability scores (unary)
        z0 = self.matchability(desc0)  # (B, M, 1)
        z1 = self.matchability(desc1)  # (B, N, 1)

        # Assignment matrix
        scores = sigmoid_log_double_softmax(sim, z0, z1)

        return scores
```

```python
def sigmoid_log_double_softmax(sim, z0, z1):
    """
    Log-domain assignment matrix computation

    æ•°å¼:
      certainties = log_sigmoid(z0) + log_sigmoid(z1)áµ€
      scores[i,j] = log_softmax(S, dim=1)[i,j]
                  + log_softmax(S, dim=0)[i,j]
                  + certainties[i,j]

      scores[:, -1] = unmatchable scores for image 0
      scores[-1, :] = unmatchable scores for image 1
    """
    b, m, n = sim.shape

    # Matchability (certainty that point has a match)
    certainties = F.logsigmoid(z0) + F.logsigmoid(z1).T

    # Double softmax in log domain
    scores0 = F.log_softmax(sim, dim=2)      # Row-wise
    scores1 = F.log_softmax(sim.T, dim=2).T  # Column-wise

    # Combined assignment (M+1 x N+1)
    scores = torch.zeros(b, m+1, n+1)
    scores[:, :m, :n] = scores0 + scores1 + certainties

    # Unmatchable scores (dustbin equivalent)
    scores[:, :-1, -1] = F.logsigmoid(-z0)  # A is unmatchable
    scores[:, -1, :-1] = F.logsigmoid(-z1)  # B is unmatchable

    return scores
```

---

### 4. [adaptive_inference.py](adaptive_inference.py)
**é©å¿œçš„æ¨è«–ï¼ˆAdaptive Depth & Widthï¼‰**

#### ğŸ”‘ **ã‚­ãƒ¼ãƒ»ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³: Adaptive Depth (Early Stopping)**

```python
class TokenConfidence(nn.Module):
    """
    å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§äºˆæ¸¬ã®ç¢ºä¿¡åº¦ã‚’æ¨å®š

    å‡¦ç†:
      1. å„ç‚¹ã®çŠ¶æ…‹ã‹ã‚‰ç¢ºä¿¡åº¦ã‚’äºˆæ¸¬
      2. ç¢ºä¿¡åº¦ãŒé«˜ã„ç‚¹ã®å‰²åˆã‚’è¨ˆç®—
      3. é–¾å€¤Î±ã‚’è¶…ãˆãŸã‚‰æ¨è«–çµ‚äº†

    æ•°å¼:
      cáµ¢ = sigmoid(MLP(xáµ¢)) âˆˆ [0, 1]

      exit = (1/(M+N) Ã— Î£[cáµ¢ > Î»â‚—]) > Î±

    where:
      Î»â‚— = 0.8 + 0.1 Ã— exp(-4â„“/L)  (å±¤ã«ã‚ˆã‚‹é–¾å€¤æ¸›è¡°)
      Î± = 0.95 (depth_confidence)
    """

    def __init__(self, dim):
        self.token = nn.Sequential(
            nn.Linear(dim, 1),
            nn.Sigmoid()
        )

    def forward(self, desc0, desc1):
        # å‹¾é…ã‚’åˆ‡æ–­ï¼ˆç¢ºä¿¡åº¦äºˆæ¸¬ã¯ãƒãƒƒãƒãƒ³ã‚°ç²¾åº¦ã«å½±éŸ¿ã•ã›ãªã„ï¼‰
        return self.token(desc0.detach()), self.token(desc1.detach())


def check_if_stop(confidences0, confidences1, layer_index, num_points):
    """
    æ—©æœŸçµ‚äº†ã®åˆ¤å®š

    åŠ¹æœ (MegaDepth):
      - Easy pairs: 3-4å±¤ã§çµ‚äº† â†’ 1.86å€é«˜é€ŸåŒ–
      - Medium pairs: 5-6å±¤ã§çµ‚äº† â†’ 1.33å€é«˜é€ŸåŒ–
      - Hard pairs: 7-9å±¤ã§çµ‚äº† â†’ 1.16å€é«˜é€ŸåŒ–
    """
    confidences = torch.cat([confidences0, confidences1], -1)
    threshold = 0.8 + 0.1 * np.exp(-4.0 * layer_index / n_layers)
    ratio_confident = (confidences >= threshold).float().mean()
    return ratio_confident > depth_confidence  # 0.95
```

#### ğŸ”‘ **ã‚­ãƒ¼ãƒ»ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³: Point Pruning (Adaptive Width)**

```python
def get_pruning_mask(confidences, matchability_scores, layer_index):
    """
    ãƒãƒƒãƒä¸å¯èƒ½ãªç‚¹ã‚’æ—©æœŸã«é™¤å¤–

    æ¡ä»¶:
      - ç¢ºä¿¡åº¦ãŒé«˜ã„ (confident)
      - ãƒãƒƒãƒå¯èƒ½æ€§ãŒä½ã„ (unmatchable)
      â†’ ä»¥é™ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‹ã‚‰é™¤å¤–

    æ•°å¼:
      unmatchable(i) = (cáµ¢ > Î»â‚—) AND (Ïƒáµ¢ < Î²)

    where:
      Î»â‚—: å±¤ä¾å­˜ã®ç¢ºä¿¡åº¦é–¾å€¤
      Î² = 0.01 (width_confidenceç›¸å½“)

    åŠ¹æœ:
      - è¨ˆç®—é‡: O(NÂ²) â†’ O((N-pruned)Â²)
      - Easy pairs: ~20%ã®pointsã‚’é™¤å¤–
      - Hard pairs: ~28%ã®pointsã‚’é™¤å¤–
    """
    keep = matchability_scores > (1 - width_confidence)  # 0.99

    # Low-confidence points are never pruned
    if confidences is not None:
        keep |= confidences <= confidence_threshold[layer_index]

    return keep  # True = keep, False = prune
```

---

### 5. [loss_computation.py](loss_computation.py)
**æå¤±é–¢æ•°**

#### ğŸ”‘ **ã‚­ãƒ¼ãƒ»ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³: Deep Supervision**

```python
class LightGlueLoss(nn.Module):
    """
    å¾“æ¥æ‰‹æ³• (SuperGlue):
      - æœ€çµ‚ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã¿ã§æå¤±è¨ˆç®—
      - Sinkhorn ãŒé‡ã„ãŸã‚ä¸­é–“å‡ºåŠ›å›°é›£

    LightGlue:
      - å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§æå¤±è¨ˆç®— (Deep Supervision)
      - è»½é‡ãªãƒ˜ãƒƒãƒ‰ã§å„å±¤ã®äºˆæ¸¬ãŒå¯èƒ½
      - åæŸãŒé€Ÿãã€ä¸­é–“ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§ã‚‚æ„å‘³ã®ã‚ã‚‹äºˆæ¸¬

    æå¤±é–¢æ•°:
      L = (1/L) Ã— Î£â‚— L_assignment(â„“)

      L_assignment = L_positive + L_negative
    """

    def forward(self, predictions, ground_truth):
        total_loss = 0

        for layer_idx in range(n_layers):
            # å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®äºˆæ¸¬ã‚’å–å¾—
            P = predictions[layer_idx]  # Assignment matrix

            # === Positive Loss (æ­£ã—ã„ãƒãƒƒãƒ) ===
            # Ground truth matches: M = {(i, j)}
            loss_positive = -log(P[i, j]) for (i, j) in matches
            loss_positive = loss_positive.mean() / |M|

            # === Negative Loss (unmatchable points) ===
            # Unmatchable in A: Ä€ = points with no correspondence
            loss_neg_A = -log(1 - Ïƒá´¬áµ¢) for i in Ä€
            loss_neg_B = -log(1 - Ïƒá´®â±¼) for j in BÌ„
            loss_negative = (loss_neg_A.mean() + loss_neg_B.mean()) / 2

            total_loss += loss_positive + loss_negative

        return total_loss / n_layers
```

#### Confidence Classifier ã®å­¦ç¿’

```python
def train_confidence_classifier(predictions):
    """
    2æ®µéšç›®ã®å­¦ç¿’: ç¢ºä¿¡åº¦åˆ†é¡å™¨

    Ground truth:
      - å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®äºˆæ¸¬ãŒæœ€çµ‚ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¨åŒã˜ã‹ï¼Ÿ
      - label_i = (match_at_layer_â„“ == match_at_layer_L)

    æå¤±:
      L_conf = BCE(cáµ¢, label_i)

    æ³¨æ„:
      - å‹¾é…ã¯çŠ¶æ…‹ã«ä¼æ’­ã•ã›ãªã„ (detach)
      - ãƒãƒƒãƒãƒ³ã‚°ç²¾åº¦ã«å½±éŸ¿ã•ã›ãªã„
    """
    for layer_idx in range(n_layers - 1):
        # å„ç‚¹ã®ãƒãƒƒãƒçµæœ
        match_at_layer = get_match(layer_idx)
        match_at_final = get_match(n_layers - 1)

        # Ground truth: äºˆæ¸¬ãŒä¸€è‡´ã—ã¦ã„ã‚‹ã‹
        labels = (match_at_layer == match_at_final).float()

        # BCE loss
        confidence = confidence_classifiers[layer_idx](desc.detach())
        loss = F.binary_cross_entropy(confidence, labels)
```

---

## LightGlueã®ä¸»è¦ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³

### 1. **Rotary Positional Encoding**
**å•é¡Œ**: SuperGlueã®çµ¶å¯¾ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¯æ·±ã„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§è–„ã‚Œã‚‹

**è§£æ±º**:
- ç›¸å¯¾ä½ç½®ã‚’ä½¿ç”¨ï¼ˆRotary encodingï¼‰
- å„self-attentionãƒ¬ã‚¤ãƒ¤ãƒ¼ã§query/keyã«é©ç”¨
- ä½ç½®æƒ…å ±ãŒå¸¸ã«ä¿æŒã•ã‚Œã‚‹

**åŠ¹æœ**:
- ç²¾åº¦: +2% precision
- å¹¾ä½•ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å­¦ç¿’ãŒå®¹æ˜“

**å®Ÿè£…**: [transformer_blocks.py](transformer_blocks.py)

---

### 2. **Bidirectional Cross-Attention**
**å•é¡Œ**: æ¨™æº–ã®cross-attentionã¯é¡ä¼¼åº¦ã‚’2å›è¨ˆç®—

**è§£æ±º**:
- Query/Keyã®æŠ•å½±ã‚’å…±æœ‰
- é¡ä¼¼åº¦è¡Œåˆ—ã‚’1å›ã ã‘è¨ˆç®—
- è»¢ç½®ã§åŒæ–¹å‘ã®attentionå–å¾—

**åŠ¹æœ**:
- è¨ˆç®—é‡: 50%å‰Šæ¸›ï¼ˆcross-attentionéƒ¨åˆ†ï¼‰
- å…¨ä½“: 20%é«˜é€ŸåŒ–
- ç²¾åº¦: å¤‰åŒ–ãªã—

**å®Ÿè£…**: [transformer_blocks.py](transformer_blocks.py)

---

### 3. **Double Softmax + Matchability**
**å•é¡Œ**: SuperGlueã®Sinkhorn Algorithmã¯é‡ã„

**è§£æ±º**:
- è¡Œæ–¹å‘ãƒ»åˆ—æ–¹å‘ã®ä¸¡Softmax
- åˆ¥é€”Matchability scoreã§ unmatchableã‚’è¡¨ç¾
- Dustbinã‚’åˆ†é›¢ã—ã¦è¡¨ç¾

**åŠ¹æœ**:
- è¨ˆç®—é‡: å¤§å¹…å‰Šæ¸›ï¼ˆ100ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ â†’ 1å›ï¼‰
- å‹¾é…: ã‚ˆã‚Šã‚¯ãƒªãƒ¼ãƒ³
- å­¦ç¿’: å®‰å®šåŒ–

**å®Ÿè£…**: [matching_head.py](matching_head.py)

---

### 4. **Deep Supervision**
**å•é¡Œ**: SuperGlueã¯æœ€çµ‚ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã¿ã§æ•™å¸«ä¿¡å·

**è§£æ±º**:
- å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§æå¤±è¨ˆç®—
- ä¸­é–“ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§ã‚‚æ„å‘³ã®ã‚ã‚‹äºˆæ¸¬ã‚’å¼·åˆ¶
- Early stoppingã®å‰ææ¡ä»¶

**åŠ¹æœ**:
- åæŸ: 3å€é«˜é€ŸåŒ–
- å­¦ç¿’ã‚³ã‚¹ãƒˆ: 2 GPU-daysï¼ˆSuperGlue: 7+ daysï¼‰

**å®Ÿè£…**: [loss_computation.py](loss_computation.py)

---

### 5. **Adaptive Depth (Early Stopping)**
**å•é¡Œ**: ç°¡å˜ãªãƒšã‚¢ã§ã‚‚å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼è¨ˆç®—ã¯ç„¡é§„

**è§£æ±º**:
- ç¢ºä¿¡åº¦åˆ†é¡å™¨ã§å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®äºˆæ¸¬ä¿¡é ¼åº¦ã‚’æ¨å®š
- ååˆ†ãªç¢ºä¿¡åº¦ã«é”ã—ãŸã‚‰æ¨è«–çµ‚äº†
- å±¤ä¾å­˜ã®é–¾å€¤ã§æ—©æœŸãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ä¸ç¢ºå®Ÿæ€§ã‚’è€ƒæ…®

**åŠ¹æœ**:
- Easy pairs: å¹³å‡4.7å±¤ã§çµ‚äº†ã€1.86å€é«˜é€ŸåŒ–
- Medium pairs: å¹³å‡5.5å±¤ã§çµ‚äº†ã€1.33å€é«˜é€ŸåŒ–
- Hard pairs: å¹³å‡6.9å±¤ã§çµ‚äº†ã€1.16å€é«˜é€ŸåŒ–

**å®Ÿè£…**: [adaptive_inference.py](adaptive_inference.py)

---

### 6. **Point Pruning (Adaptive Width)**
**å•é¡Œ**: ãƒãƒƒãƒä¸å¯èƒ½ãªç‚¹ã‚‚å¾Œç¶šãƒ¬ã‚¤ãƒ¤ãƒ¼ã§å‡¦ç†

**è§£æ±º**:
- æ—©æœŸã«ãƒãƒƒãƒä¸å¯èƒ½ã¨åˆ¤æ–­ã•ã‚ŒãŸç‚¹ã‚’é™¤å¤–
- Attentionã®è¨ˆç®—é‡O(NÂ²)ã‚’å‰Šæ¸›
- æ¢ç´¢ç©ºé–“ã‚’ç¸®å°

**åŠ¹æœ**:
- å¹³å‡23.7%ã®ç‚¹ã‚’é™¤å¤–
- ç‰¹ã«Hard pairsï¼ˆä½overlapï¼‰ã§åŠ¹æœçš„
- è¨ˆç®—é‡å‰Šæ¸› + ç²¾åº¦ç¶­æŒ

**å®Ÿè£…**: [adaptive_inference.py](adaptive_inference.py)

---

## SuperGlueã¨ã®æ¯”è¼ƒ

| å´é¢ | SuperGlue | LightGlue |
|------|-----------|-----------|
| **ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°** | çµ¶å¯¾ä½ç½® (MLP) | ç›¸å¯¾ä½ç½® (Rotary) |
| **Cross-Attention** | æ¨™æº– (2å›è¨ˆç®—) | åŒæ–¹å‘ (1å›è¨ˆç®—) |
| **Assignment** | Sinkhorn (100 iter) | Double Softmax |
| **Unmatchable** | Dustbin (entangled) | Matchability (separated) |
| **æ•™å¸«ä¿¡å·** | æœ€çµ‚å±¤ã®ã¿ | å…¨å±¤ (Deep Supervision) |
| **é©å¿œæ€§** | ãªã— | Depth + Width |
| **å­¦ç¿’æ™‚é–“** | 7+ GPU-days | 2 GPU-days |
| **æ¨è«–é€Ÿåº¦** | 70ms | 31-44ms |
| **ç²¾åº¦ (AUC@5Â°)** | 49.7% | 49.4-49.9% |

---

## å‡¦ç†ãƒ•ãƒ­ãƒ¼è©³ç´°

### æ¨è«–ãƒ•ãƒ­ãƒ¼

```python
# 1. å…¥åŠ›ãƒ‡ãƒ¼ã‚¿æº–å‚™
data = {
    'image0': {
        'keypoints': torch.randn(B, M, 2),      # (B, M, 2) ãƒ”ã‚¯ã‚»ãƒ«åº§æ¨™
        'descriptors': torch.randn(B, M, 256),  # (B, M, D) è¨˜è¿°å­
        'image_size': torch.tensor([H, W])      # ç”»åƒã‚µã‚¤ã‚º
    },
    'image1': {
        'keypoints': torch.randn(B, N, 2),
        'descriptors': torch.randn(B, N, 256),
        'image_size': torch.tensor([H, W])
    }
}

# 2. ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ­£è¦åŒ–
kpts0 = normalize_keypoints(kpts0, size0)  # [-1, 1]ã«æ­£è¦åŒ–
kpts1 = normalize_keypoints(kpts1, size1)

# 3. å…¥åŠ›æŠ•å½±
desc0 = input_proj(desc0)  # (B, M, 256)
desc1 = input_proj(desc1)  # (B, N, 256)

# 4. ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
encoding0 = posenc(kpts0)  # (2, B, M, head_dim)
encoding1 = posenc(kpts1)  # (2, B, N, head_dim)

# 5. Transformerãƒ¬ã‚¤ãƒ¤ãƒ¼
for i in range(9):  # L=9 layers
    # Self-attention (with Rotary PE)
    desc0 = self_attn(desc0, encoding0)
    desc1 = self_attn(desc1, encoding1)

    # Cross-attention (bidirectional)
    desc0, desc1 = cross_attn(desc0, desc1)

    if i < 8:  # Last layer: no early stop
        # Confidence check
        token0, token1 = token_confidence[i](desc0, desc1)
        if check_if_stop(token0, token1, i, M + N):
            break

        # Point pruning
        if desc0.shape[1] > 1024:
            keep0 = get_pruning_mask(token0, matchability0, i)
            desc0 = desc0[:, keep0]
            encoding0 = encoding0[..., keep0, :]

# 6. Matching head
scores, sim = log_assignment[i](desc0, desc1)
# scores: (B, M+1, N+1) log assignment matrix

# 7. Match filtering
m0, m1, mscores0, mscores1 = filter_matches(scores, threshold=0.1)

# 8. å‡ºåŠ›
output = {
    'matches0': m0,           # (B, M) å„ç‚¹ã®ãƒãƒƒãƒå…ˆ (-1 = unmatched)
    'matches1': m1,           # (B, N)
    'matching_scores0': mscores0,  # (B, M) ãƒãƒƒãƒä¿¡é ¼åº¦
    'matching_scores1': mscores1,  # (B, N)
    'matches': matches,       # List[(Si, 2)] ãƒãƒƒãƒã”ã¨ã®ãƒãƒƒãƒãƒšã‚¢
    'scores': mscores,        # List[(Si,)] ãƒãƒƒãƒã‚¹ã‚³ã‚¢
    'stop': i + 1,            # çµ‚äº†ãƒ¬ã‚¤ãƒ¤ãƒ¼
    'prune0': prune0,         # (B, M) pruning layer
    'prune1': prune1          # (B, N)
}
```

---

### è¨“ç·´ãƒ•ãƒ­ãƒ¼

```python
# === Stage 1: ãƒãƒƒãƒãƒ³ã‚°äºˆæ¸¬ã®å­¦ç¿’ ===

for epoch in range(50):
    for batch in dataloader:
        # Forward pass
        predictions = model(batch)

        # Ground truth from reprojection
        matches_gt = compute_ground_truth_matches(
            kpts0, kpts1,
            homography=batch.get('H'),
            depth=batch.get('depth'),
            pose=batch.get('pose')
        )

        # Loss at each layer
        total_loss = 0
        for layer_idx in range(n_layers):
            P = predictions['scores_at_layer'][layer_idx]

            # Positive: correct matches
            loss_pos = -log(P[matches_gt]).mean()

            # Negative: unmatchable points
            loss_neg = -log(1 - matchability[unmatchable]).mean()

            total_loss += (loss_pos + loss_neg) / n_layers

        total_loss.backward()
        optimizer.step()

# === Stage 2: ç¢ºä¿¡åº¦åˆ†é¡å™¨ã®å­¦ç¿’ ===

# ãƒãƒƒãƒãƒ³ã‚°éƒ¨åˆ†ã®é‡ã¿ã‚’å›ºå®š
for param in model.transformers.parameters():
    param.requires_grad = False
for param in model.log_assignment.parameters():
    param.requires_grad = False

for epoch in range(10):
    for batch in dataloader:
        predictions = model(batch)

        # Ground truth: å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®äºˆæ¸¬ãŒæœ€çµ‚ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¨åŒã˜ã‹
        for layer_idx in range(n_layers - 1):
            match_at_layer = get_match_at_layer(predictions, layer_idx)
            match_at_final = get_match_at_layer(predictions, n_layers - 1)
            labels = (match_at_layer == match_at_final).float()

            confidence = token_confidence[layer_idx](desc.detach())
            loss = F.binary_cross_entropy(confidence, labels)

        loss.backward()
        optimizer.step()
```

**è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**:
- **Pre-training**: Oxford-Paris 1M (synthetic homographies)
- **Fine-tuning**: MegaDepth (196 landmarks, 1M images)

**è¨“ç·´è¨­å®š**:
- Batch size: 32
- Keypoints: 2048 per image
- Learning rate: 1e-4 (pre-train), 1e-5 (fine-tune)
- GPU: 2Ã— RTX 3090
- Time: ~2 GPU-days

---

## å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

### 1. Homographic Dataset (Pre-training)

```python
# åˆæˆHomography ã«ã‚ˆã‚‹è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’
class HomographicDataset:
    """
    ç‰¹å¾´:
    - 1æšã®ç”»åƒã‹ã‚‰åˆæˆãƒšã‚¢ç”Ÿæˆ
    - å®Œå…¨ãªground truthï¼ˆãƒã‚¤ã‚ºãªã—ï¼‰
    - æ¥µç«¯ãªå¤‰æ›ã‚‚å¯èƒ½

    ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿:
    - Oxford-Paris 1M distractors (170K images)
    """

    def __getitem__(self, idx):
        image = load_image(self.paths[idx])

        # Random homography
        H = sample_random_homography()

        # Warp image
        image_b = warp_perspective(image, H)

        # Extract features
        kpts_a, desc_a = extractor(image)
        kpts_b, desc_b = extractor(image_b)

        return {
            'image0': {'keypoints': kpts_a, 'descriptors': desc_a},
            'image1': {'keypoints': kpts_b, 'descriptors': desc_b},
            'H': H  # Ground truth transformation
        }
```

### 2. MegaDepth Dataset (Fine-tuning)

```python
# å®Ÿç”»åƒãƒšã‚¢ã«ã‚ˆã‚‹æ•™å¸«ã‚ã‚Šå­¦ç¿’
class MegaDepthDataset:
    """
    ç‰¹å¾´:
    - å®Ÿéš›ã®ç”»åƒãƒšã‚¢
    - SfM + MVS ã«ã‚ˆã‚‹ depth/pose
    - ç¾å®Ÿçš„ãªå¤‰åŒ–ã‚’å«ã‚€

    ãƒ‡ãƒ¼ã‚¿æ§‹æˆ:
    - 196 landmarks
    - 1M crowd-sourced images
    - Scene splits for train/val/test
    """

    def __getitem__(self, idx):
        scene_info = load_scene_info(self.scenes[idx])

        # Sample pair by covisibility
        img_a, img_b = sample_pair_by_overlap(scene_info)

        # Load depth and poses
        depth_a = load_depth(scene_info, img_a)
        pose_a = scene_info['poses'][img_a]
        pose_b = scene_info['poses'][img_b]
        K_a = scene_info['intrinsics'][img_a]
        K_b = scene_info['intrinsics'][img_b]

        # Extract features
        kpts_a, desc_a = extractor(img_a)
        kpts_b, desc_b = extractor(img_b)

        return {
            'image0': {'keypoints': kpts_a, 'descriptors': desc_a},
            'image1': {'keypoints': kpts_b, 'descriptors': desc_b},
            'depth0': depth_a,
            'K0': K_a, 'K1': K_b,
            'T_0to1': pose_b @ inv(pose_a)
        }
```

### Ground Truth ãƒãƒƒãƒã®è¨ˆç®—

```python
def compute_ground_truth_matches(kpts_a, kpts_b, H=None, depth=None, pose=None):
    """
    Homography ã®å ´åˆ:
        kpts_a_warped = H @ kpts_a
        match if ||kpts_a_warped - kpts_b|| < 3px

    Depth + Pose ã®å ´åˆ:
        p_cam_a = K_a^-1 @ kpts_a * depth_a
        p_cam_b = R @ p_cam_a + t
        kpts_a_warped = K_b @ p_cam_b
        match if ||kpts_a_warped - kpts_b|| < 3px

    Unmatchable:
        - Reprojection error > 5px for all points
        - No depth available
        - Large epipolar error
    """
    # ... implementation
    return matches, unmatchable_A, unmatchable_B
```

---

## å½¢çŠ¶ã‚¬ã‚¤ãƒ‰

### å…¥åŠ›ãƒ»ä¸­é–“ãƒ»å‡ºåŠ›å½¢çŠ¶

| æ®µéš | åç§° | å½¢çŠ¶ | èª¬æ˜ |
|------|------|------|------|
| **å…¥åŠ›** | keypoints | `(B, M/N, 2)` | ãƒ”ã‚¯ã‚»ãƒ«åº§æ¨™ |
| | descriptors | `(B, M/N, D)` | D=128 or 256 |
| | image_size | `(B, 2)` or `(2,)` | [H, W] |
| **æ­£è¦åŒ–** | kpts_norm | `(B, M/N, 2)` | [-1, 1]æ­£è¦åŒ– |
| **æŠ•å½±å¾Œ** | desc | `(B, M/N, 256)` | çµ±ä¸€æ¬¡å…ƒ |
| **Rotary PE** | encoding | `(2, B, M/N, head_dim)` | cos/sin |
| **Self-Attnå†…** | qkv | `(B, H, M/N, 3, head_dim)` | H=4 heads |
| **Cross-Attnå†…** | sim | `(B, H, M, N)` | é¡ä¼¼åº¦è¡Œåˆ— |
| **Confidence** | token | `(B, M/N)` | [0, 1] |
| **Matchability** | Ïƒ | `(B, M/N)` | [0, 1] |
| **Assignment** | scores | `(B, M+1, N+1)` | logç¢ºç‡ |
| **å‡ºåŠ›** | matches0 | `(B, M)` | ãƒãƒƒãƒå…ˆindex |
| | matches | `List[(Si, 2)]` | ãƒšã‚¢ãƒªã‚¹ãƒˆ |

### è»¸ã®æ„å‘³

- **B**: ãƒãƒƒãƒã‚µã‚¤ã‚º
- **M**: Image Aã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ•°
- **N**: Image Bã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ•°
- **D**: å…¥åŠ›è¨˜è¿°å­æ¬¡å…ƒ (128 for DISK/ALIKED, 256 for SuperPoint)
- **256**: å†…éƒ¨çŠ¶æ…‹æ¬¡å…ƒ (descriptor_dim)
- **H**: Attention heads (4)
- **head_dim**: 64 (= 256 / 4)
- **L**: ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•° (9)

---

## FAQ

### Q1: LightGlueã¨SuperGlueã®æœ€å¤§ã®é•ã„ã¯ï¼Ÿ

**A**: 3ã¤ã®ä¸»è¦ãªé•ã„ãŒã‚ã‚Šã¾ã™ã€‚

1. **Assignmentæ–¹æ³•**
   - SuperGlue: Sinkhorn Algorithmï¼ˆæœ€é©è¼¸é€ã€100ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
   - LightGlue: Double Softmax + Matchabilityï¼ˆ1å›ã®è¨ˆç®—ï¼‰

   ```python
   # SuperGlue
   for _ in range(100):
       scores = scores - scores.logsumexp(1)
       scores = scores - scores.logsumexp(0)

   # LightGlue
   scores = log_softmax(sim, dim=1) + log_softmax(sim, dim=0) + certainties
   ```

2. **é©å¿œæ€§**
   - SuperGlue: å›ºå®šæ·±åº¦ï¼ˆ9å±¤å…¨ã¦å®Ÿè¡Œï¼‰
   - LightGlue: Adaptive depth + width

   ```python
   # LightGlue
   for i in range(9):
       desc0, desc1 = transformer[i](desc0, desc1)
       if check_if_stop(confidence, i):
           break  # Early exit!
       desc0 = desc0[keep_mask]  # Point pruning!
   ```

3. **å­¦ç¿’åŠ¹ç‡**
   - SuperGlue: 7+ GPU-days, æœ€çµ‚å±¤ã®ã¿æ•™å¸«ä¿¡å·
   - LightGlue: 2 GPU-days, Deep Supervision

---

### Q2: Rotary Positional Encodingã®åˆ©ç‚¹ã¯ï¼Ÿ

**A**: 3ã¤ã®åˆ©ç‚¹ãŒã‚ã‚Šã¾ã™ã€‚

1. **ç›¸å¯¾ä½ç½®ã®è¡¨ç¾**
   ```python
   # çµ¶å¯¾ä½ç½® (SuperGlue)
   attention = softmax(q @ k.T)  # ä½ç½®æƒ…å ±ãªã—

   # ç›¸å¯¾ä½ç½® (LightGlue)
   attention = softmax(q @ R(p_j - p_i) @ k.T)  # ç›¸å¯¾ä½ç½®ã‚’è€ƒæ…®
   ```

2. **ä½ç½®æƒ…å ±ã®ä¿æŒ**
   - å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§å†é©ç”¨
   - æ·±ã„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§ã‚‚ä½ç½®ã‚’å‚ç…§å¯èƒ½

3. **å¹¾ä½•ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å­¦ç¿’**
   - ã€Œå³ä¸Šã«ã‚ã‚‹ç‚¹ã€ã€Œè¿‘ãã®ç‚¹ã€ãªã©ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
   - ç”»åƒé–“ã§æ¯”è¼ƒå¯èƒ½ãªè¡¨ç¾

---

### Q3: Adaptive Depthã¯ã©ã†æ©Ÿèƒ½ã™ã‚‹ï¼Ÿ

**A**: ç¢ºä¿¡åº¦ãƒ™ãƒ¼ã‚¹ã®æ—©æœŸçµ‚äº†ã§ã™ã€‚

```
Layer 1: 50%ã®ç‚¹ãŒç¢ºä¿¡åº¦é«˜ã„ â†’ ç¶šè¡Œ
Layer 2: 60%ã®ç‚¹ãŒç¢ºä¿¡åº¦é«˜ã„ â†’ ç¶šè¡Œ
Layer 3: 80%ã®ç‚¹ãŒç¢ºä¿¡åº¦é«˜ã„ â†’ ç¶šè¡Œ
Layer 4: 96%ã®ç‚¹ãŒç¢ºä¿¡åº¦é«˜ã„ â†’ åœæ­¢! (>95%é–¾å€¤)
```

**åŠ¹æœã®ä¾‹**:
| ãƒšã‚¢ã‚¿ã‚¤ãƒ— | å¹³å‡åœæ­¢å±¤ | é€Ÿåº¦å‘ä¸Š |
|-----------|-----------|---------|
| Easy (é«˜overlap) | 4.7å±¤ | 1.86å€ |
| Medium | 5.5å±¤ | 1.33å€ |
| Hard (ä½overlap) | 6.9å±¤ | 1.16å€ |

---

### Q4: Point Pruningã®åˆ¤å®šåŸºæº–ã¯ï¼Ÿ

**A**: 2ã¤ã®æ¡ä»¶ã®çµ„ã¿åˆã‚ã›ã§ã™ã€‚

```python
prune_point = (confidence > threshold) AND (matchability < 0.01)
```

1. **ç¢ºä¿¡åº¦ãŒé«˜ã„**: äºˆæ¸¬ãŒå®‰å®šã—ã¦ã„ã‚‹
2. **ãƒãƒƒãƒå¯èƒ½æ€§ãŒä½ã„**: ãƒãƒƒãƒç›¸æ‰‹ãŒã„ãªã„

**ã©ã‚“ãªç‚¹ãŒé™¤å¤–ã•ã‚Œã‚‹ï¼Ÿ**
- ã‚ªã‚¯ãƒ«ãƒ¼ã‚¸ãƒ§ãƒ³é ˜åŸŸ
- è¦–é‡å¤–
- ãƒ†ã‚¯ã‚¹ãƒãƒ£ãƒ¬ã‚¹é ˜åŸŸ
- å‹•çš„ç‰©ä½“

---

### Q5: å­¦ç¿’ã®2æ®µéšã¨ã¯ï¼Ÿ

**A**: ãƒãƒƒãƒãƒ³ã‚°äºˆæ¸¬ã¨ç¢ºä¿¡åº¦äºˆæ¸¬ã‚’åˆ†é›¢ã—ã¦å­¦ç¿’ã—ã¾ã™ã€‚

**Stage 1: ãƒãƒƒãƒãƒ³ã‚°äºˆæ¸¬**
```python
# ç›®çš„: æ­£ã—ã„å¯¾å¿œé–¢ä¿‚ã‚’äºˆæ¸¬
loss = -log(P[gt_matches]) - log(1 - Ïƒ[unmatchable])
```

**Stage 2: ç¢ºä¿¡åº¦åˆ†é¡å™¨**
```python
# ç›®çš„: å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®äºˆæ¸¬ãŒæœ€çµ‚å±¤ã¨åŒã˜ã‹äºˆæ¸¬
# é‡è¦: ãƒãƒƒãƒãƒ³ã‚°éƒ¨åˆ†ã¯å›ºå®šï¼ˆå‹¾é…ã‚’ä¼æ’­ã•ã›ãªã„ï¼‰
label = (match_at_layer_i == match_at_final_layer)
loss = BCE(confidence, label)
```

**ãªãœåˆ†é›¢ï¼Ÿ**
- ç¢ºä¿¡åº¦äºˆæ¸¬ã¯ãƒãƒƒãƒãƒ³ã‚°ç²¾åº¦ã«å½±éŸ¿ã•ã›ãŸããªã„
- ç¢ºä¿¡åº¦ã¯ã€Œæ—©æœŸçµ‚äº†ã®åˆ¤å®šã€ã«ã®ã¿ä½¿ç”¨
- Stage 1ã®åæŸå¾Œã«Stage 2ã‚’å­¦ç¿’

---

### Q6: å¯¾å¿œå¯èƒ½ãªç‰¹å¾´é‡ã¯ï¼Ÿ

**A**: è¤‡æ•°ã®å±€æ‰€ç‰¹å¾´é‡ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚

| ç‰¹å¾´é‡ | å…¥åŠ›æ¬¡å…ƒ | è¿½åŠ æƒ…å ± | ç”¨é€” |
|--------|---------|---------|------|
| SuperPoint | 256 | ãªã— | ä¸€èˆ¬ç”¨é€” |
| DISK | 128 | ãªã— | é«˜ç²¾åº¦ |
| ALIKED | 128 | ãªã— | é«˜é€Ÿ |
| SIFT | 128 | scale, orientation | å¤å…¸çš„ |
| DoG-HardNet | 128 | scale, orientation | ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ |

```python
# ä½¿ç”¨ä¾‹
model = LightGlue(features='superpoint')  # or 'disk', 'aliked', 'sift'

# ã‚«ã‚¹ã‚¿ãƒ ç‰¹å¾´é‡
model = LightGlue(features=None, input_dim=128)
```

---

### Q7: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ã®ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã¯ï¼Ÿ

**A**: è¤‡æ•°ã®æ‰‹æ³•ã‚’çµ„ã¿åˆã‚ã›ã¦ã„ã¾ã™ã€‚

1. **FlashAttention**
   ```python
   # æ¨™æº–attention: O(NÂ²)ãƒ¡ãƒ¢ãƒª
   # FlashAttention: O(N)ãƒ¡ãƒ¢ãƒª
   if FLASH_AVAILABLE:
       v = F.scaled_dot_product_attention(q, k, v)
   ```

2. **Gradient Checkpointing**
   ```python
   # è¨“ç·´æ™‚ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
   # Forwardæ™‚ã«ä¸­é–“çµæœã‚’ä¿å­˜ã—ãªã„
   # Backwardæ™‚ã«å†è¨ˆç®—
   ```

3. **Mixed Precision**
   ```python
   with torch.autocast(device_type='cuda'):
       output = model(data)
   ```

4. **Point Pruning**
   - ä¸è¦ãªç‚¹ã‚’é™¤å¤–
   - O(NÂ²) â†’ O((N-pruned)Â²)

---

### Q8: Dense Matcherã¨ã®æ¯”è¼ƒã¯ï¼Ÿ

**A**: é€Ÿåº¦ã§å¤§ããå„ªä½ã€ç²¾åº¦ã¯åŒç­‰ãƒ¬ãƒ™ãƒ«ã§ã™ã€‚

| Method | Type | AUC@5Â° | Time (ms) | Speed |
|--------|------|--------|-----------|-------|
| LoFTR | Dense | 52.8 | 181 | 5.5 fps |
| MatchFormer | Dense | 53.3 | 388 | 2.6 fps |
| ASpanFormer | Dense | 55.3 | 369 | 2.7 fps |
| **LightGlue** | Sparse | 49.9 | 44 | **22.7 fps** |
| LightGlue (adaptive) | Sparse | 49.4 | 31 | **32.3 fps** |

**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**:
- Dense: é«˜ç²¾åº¦ã€ä½é€Ÿã€ãƒ¡ãƒ¢ãƒªå¤§
- Sparse (LightGlue): ã‚„ã‚„ä½ç²¾åº¦ã€é«˜é€Ÿã€ãƒ¡ãƒ¢ãƒªå°

**LightGlueã®å¼·ã¿**:
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
- å¤§è¦æ¨¡SfM/SLAM
- ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹

---

### Q9: LightGlueã®é™ç•Œã¯ï¼Ÿ

**A**: ä¸»ã«ä»¥ä¸‹ã®åˆ¶é™ãŒã‚ã‚Šã¾ã™ã€‚

1. **Sparseç‰¹å¾´é‡ã¸ã®ä¾å­˜**
   - ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ¤œå‡ºã®å“è³ªã«ä¾å­˜
   - ãƒ†ã‚¯ã‚¹ãƒãƒ£ãƒ¬ã‚¹é ˜åŸŸã§ã¯å›°é›£

2. **æ¥µç«¯ãªå¤‰åŒ–ã¸ã®å¯¾å¿œ**
   - å¤§ããªã‚¹ã‚±ãƒ¼ãƒ«å¤‰åŒ–ï¼ˆ4å€ä»¥ä¸Šï¼‰
   - æ¥µç«¯ãªè¦–ç‚¹å¤‰åŒ–

3. **ç¹°ã‚Šè¿”ã—ãƒ‘ã‚¿ãƒ¼ãƒ³**
   - å»ºç‰©ã®ãƒ•ã‚¡ã‚µãƒ¼ãƒ‰
   - ã‚¿ã‚¤ãƒ«çŠ¶ã®ãƒ†ã‚¯ã‚¹ãƒãƒ£
   â†’ èª¤ãƒãƒƒãƒãŒç™ºç”Ÿã—ã‚„ã™ã„

**å¯¾ç­–**:
- Multi-scaleç‰¹å¾´é‡ã®ä½¿ç”¨
- ã‚ˆã‚Šå¼·åŠ›ãªç‰¹å¾´é‡ï¼ˆDISK, ALIKEDï¼‰
- å¾Œå‡¦ç†ï¼ˆRANSAC, MAGSACï¼‰

---

### Q10: æ¨å¥¨è¨­å®šã¯ï¼Ÿ

**A**: ç”¨é€”ã«å¿œã˜ãŸè¨­å®šã‚’æ¨å¥¨ã—ã¾ã™ã€‚

**ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ SLAM**:
```python
model = LightGlue(
    features='superpoint',
    n_layers=9,
    depth_confidence=0.95,  # Adaptive depth ON
    width_confidence=0.99,  # Point pruning ON
    flash=True
)
# 30+ fps @ 2048 keypoints
```

**é«˜ç²¾åº¦SfM**:
```python
model = LightGlue(
    features='disk',  # or 'aliked'
    n_layers=9,
    depth_confidence=-1,   # Adaptive depth OFF
    width_confidence=-1,   # Point pruning OFF
    flash=True
)
# 20+ fps @ 2048 keypoints, highest accuracy
```

**ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹**:
```python
model = LightGlue(
    features='aliked',
    n_layers=5,            # Reduced layers
    depth_confidence=0.90,  # Aggressive early stopping
    width_confidence=0.95,  # Aggressive pruning
    flash=False            # CPU inference
)
# Real-time on mobile
```

---

## ã¾ã¨ã‚

LightGlueã¯ä»¥ä¸‹ã®6ã¤ã®ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã§SuperGlueã‚’è¶…è¶Š:

1. **Rotary PE**: ç›¸å¯¾ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ä½ç½®æƒ…å ±ã‚’ä¿æŒ
2. **Bidirectional Cross-Attention**: é¡ä¼¼åº¦è¨ˆç®—ã‚’åŠæ¸›
3. **Double Softmax + Matchability**: Sinkhornã‚’ç½®ãæ›ãˆ
4. **Deep Supervision**: å…¨å±¤ã§æ•™å¸«ä¿¡å·ã€åæŸé«˜é€ŸåŒ–
5. **Adaptive Depth**: ç°¡å˜ãªãƒšã‚¢ã¯æ—©æœŸçµ‚äº†
6. **Point Pruning**: ä¸è¦ãªç‚¹ã‚’é™¤å¤–

**æ€§èƒ½**:
- é€Ÿåº¦: SuperGlueã®2.5å€ï¼ˆadaptiveæ™‚ï¼‰
- ç²¾åº¦: åŒç­‰ä»¥ä¸Š
- å­¦ç¿’: 3å€é«˜é€ŸåŒ–ï¼ˆ2 GPU-daysï¼‰

**ç”¨é€”**:
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ SLAMï¼ˆ30+ fpsï¼‰
- å¤§è¦æ¨¡3Då†æ§‹æˆ
- Visual Localization
- ç”»åƒæ¤œç´¢ãƒ»ç…§åˆ

---

## å‚è€ƒæ–‡çŒ®

- è«–æ–‡: [LightGlue: Local Feature Matching at Light Speed](https://arxiv.org/abs/2306.13643)
- å…¬å¼ã‚³ãƒ¼ãƒ‰: [github.com/cvg/LightGlue](https://github.com/cvg/LightGlue)
- é–¢é€£ç ”ç©¶:
  - SuperGlue (2020): å…ƒç¥–Deep Matcher
  - LoFTR (2021): Dense Matcher
  - RoFormer (2021): Rotary Position Embedding
  - FlashAttention (2022): åŠ¹ç‡çš„Attention

---

**Note**: ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç¾¤ã¯ç†è§£ã‚’ç›®çš„ã¨ã—ãŸç°¡ç•¥åŒ–ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ã§ã™ã€‚å®Ÿéš›ã®å®Ÿè£…ã¨ã¯ç•°ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
